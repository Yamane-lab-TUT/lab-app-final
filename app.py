# --------------------------------------------------------------------------
# Yamane Lab Convenience Tool - Streamlit Application (app.py)
#
# v20.6.1 (PLæ³¢é•·æ ¡æ­£å¯¾å¿œç‰ˆ)
# - NEW: load_pl_data(uploaded_file) é–¢æ•°ã‚’è¿½åŠ ã—ã€ãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ åã‚’ 'pixel', 'intensity' ã«å›ºå®šã€‚
# - CHG: page_pl_analysis() ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›ã®æ³¢é•·æ ¡æ­£ãƒ­ã‚¸ãƒƒã‚¯ã«ç½®ãæ›ãˆã€æ ¡æ­£ä¿‚æ•°ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã§ä¿æŒã€‚
# - FIX: å…¨ã¦ã®ãƒªã‚¹ãƒˆã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–‹å§‹æ—¥ã‚’2025/4/1ã«è¨­å®š (v20.6.0ã‹ã‚‰å¤‰æ›´ãªã—)ã€‚
# --------------------------------------------------------------------------
# [FIXED BY GEMINI] IVãƒ‡ãƒ¼ã‚¿è§£æãƒ­ã‚¸ãƒƒã‚¯ã‚’å®‰å®šç‰ˆã«ç½®ãæ›ãˆ (UnboundLocalError, 10ãƒ•ã‚¡ã‚¤ãƒ«åˆ¶é™å¯¾å¿œ)
# --------------------------------------------------------------------------

import streamlit as st
import gspread
import pandas as pd
import os 
import io
import re
import json
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime, date, timedelta
from urllib.parse import quote as url_quote
from io import BytesIO
import calendar
import matplotlib.font_manager as fm

# GCSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ 
try:
    from google.cloud import storage
except ImportError:
    st.error("âŒ è­¦å‘Š: `google-cloud-storage` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    pass
    
# --- Matplotlib æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š ---
try:
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['font.sans-serif'] = ['Hiragino Maru Gothic Pro', 'Yu Gothic', 'Meiryo', 'TakaoGothic', 'IPAexGothic', 'IPAfont', 'Noto Sans CJK JP']
    plt.rcParams['axes.unicode_minus'] = False
except Exception:
    pass
    
# --- Global Configuration & Setup ---
st.set_page_config(page_title="å±±æ ¹ç ” ä¾¿åˆ©å±‹ã•ã‚“", layout="wide")

# â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
# â†“â†“â†“â†“â†“â†“ ã€é‡è¦ã€‘ã”è‡ªèº«ã®ã€Œãƒã‚±ãƒƒãƒˆåã€ã«æ›¸ãæ›ãˆã¦ãã ã•ã„ â†“â†“â†“â†“â†“â†“
CLOUD_STORAGE_BUCKET_NAME = "yamane-lab-app-files" 
# â†‘â†‘â†‘â†‘â†‘â†‘ ã€é‡è¦ã€‘ã”è‡ªèº«ã®ã€Œãƒã‚±ãƒƒãƒˆåã€ã«æ›¸ãæ›ãˆã¦ãã ã•ã„ â†‘â†‘â†‘â†‘â†‘â†‘
# â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
MAX_COMBINED_FILES = 10 # çµåˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€å¤§æ•° [NEW]

SPREADSHEET_NAME = 'ã‚¨ãƒ”ãƒãƒ¼ãƒˆ' # Google Spreadsheetã®ãƒ•ã‚¡ã‚¤ãƒ«å

# --- SPREADSHEET COLUMN HEADERS (ãŠå®¢æ§˜ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å®Œå…¨ä¸€è‡´) ---

SHEET_EPI_DATA = 'ã‚¨ãƒ”ãƒãƒ¼ãƒˆ_ãƒ‡ãƒ¼ã‚¿'
EPI_COL_TIMESTAMP = 'ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—'
EPI_COL_NOTE_TYPE = 'ãƒãƒ¼ãƒˆç¨®åˆ¥'
EPI_COL_CATEGORY = 'ã‚«ãƒ†ã‚´ãƒª'
EPI_COL_MEMO = 'ãƒ¡ãƒ¢' # ã‚¿ã‚¤ãƒˆãƒ«ã¨è©³ç´°ãƒ¡ãƒ¢ã‚’å«ã‚€
EPI_COL_FILENAME = 'ãƒ•ã‚¡ã‚¤ãƒ«å'
EPI_COL_FILE_URL = 'å†™çœŸURL'

SHEET_MAINTE_DATA = 'ãƒ¡ãƒ³ãƒ†ãƒãƒ¼ãƒˆ_ãƒ‡ãƒ¼ã‚¿'
MAINT_COL_TIMESTAMP = 'ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—'
MAINT_COL_NOTE_TYPE = 'ãƒãƒ¼ãƒˆç¨®åˆ¥'
MAINT_COL_MEMO = 'ãƒ¡ãƒ¢'
MAINT_COL_FILENAME = 'ãƒ•ã‚¡ã‚¤ãƒ«å'
MAINT_COL_FILE_URL = 'å†™çœŸURL'

SHEET_MEETING_DATA = 'è­°äº‹éŒ²_ãƒ‡ãƒ¼ã‚¿'
MEETING_COL_TIMESTAMP = 'ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—'
MEETING_COL_TITLE = 'ä¼šè­°ã‚¿ã‚¤ãƒˆãƒ«'
MEETING_COL_AUDIO_NAME = 'éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«å'
MEETING_COL_AUDIO_URL = 'éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«URL'
MEETING_COL_CONTENT = 'è­°äº‹éŒ²å†…å®¹'

SHEET_HANDOVER_DATA = 'å¼•ãç¶™ã_ãƒ‡ãƒ¼ã‚¿'
HANDOVER_COL_TIMESTAMP = 'ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—'
HANDOVER_COL_TYPE = 'ç¨®é¡'
HANDOVER_COL_TITLE = 'ã‚¿ã‚¤ãƒˆãƒ«'
HANDOVER_COL_MEMO = 'ãƒ¡ãƒ¢' # å†…å®¹1,2,3ã¯UIã‚’è¤‡é›‘ã«ã™ã‚‹ãŸã‚ã€ä¸€æ—¦ãƒ¡ãƒ¢ã«çµ±åˆ

SHEET_QA_DATA = 'çŸ¥æµè¢‹_ãƒ‡ãƒ¼ã‚¿'
QA_COL_TIMESTAMP = 'ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—'
QA_COL_TITLE = 'è³ªå•ã‚¿ã‚¤ãƒˆãƒ«'
QA_COL_CONTENT = 'è³ªå•å†…å®¹'
QA_COL_CONTACT = 'é€£çµ¡å…ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'
QA_COL_FILENAME = 'æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«å'
QA_COL_FILE_URL = 'æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«URL'
QA_COL_STATUS = 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹'
SHEET_QA_ANSWER = 'çŸ¥æµè¢‹_è§£ç­”' # è§£ç­”ã‚·ãƒ¼ãƒˆ

SHEET_CONTACT_DATA = 'ãŠå•ã„åˆã‚ã›_ãƒ‡ãƒ¼ã‚¿'
CONTACT_COL_TIMESTAMP = 'ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—'
CONTACT_COL_TYPE = 'ãŠå•ã„åˆã‚ã›ã®ç¨®é¡'
CONTACT_COL_DETAIL = 'è©³ç´°å†…å®¹'
CONTACT_COL_CONTACT = 'é€£çµ¡å…ˆ'

SHEET_TROUBLE_DATA = 'ãƒˆãƒ©ãƒ–ãƒ«å ±å‘Š_ãƒ‡ãƒ¼ã‚¿'
TROUBLE_COL_TIMESTAMP = 'ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—'
TROUBLE_COL_DEVICE = 'æ©Ÿå™¨/å ´æ‰€'
TROUBLE_COL_OCCUR_DATE = 'ç™ºç”Ÿæ—¥'
TROUBLE_COL_OCCUR_TIME = 'ãƒˆãƒ©ãƒ–ãƒ«ç™ºç”Ÿæ™‚'
TROUBLE_COL_CAUSE = 'åŸå› /ç©¶æ˜'
TROUBLE_COL_SOLUTION = 'å¯¾ç­–/å¾©æ—§'
TROUBLE_COL_PREVENTION = 'å†ç™ºé˜²æ­¢ç­–'
TROUBLE_COL_REPORTER = 'å ±å‘Šè€…'
TROUBLE_COL_FILENAME = 'ãƒ•ã‚¡ã‚¤ãƒ«å'
TROUBLE_COL_FILE_URL = 'ãƒ•ã‚¡ã‚¤ãƒ«URL'
TROUBLE_COL_TITLE = 'ä»¶å/ã‚¿ã‚¤ãƒˆãƒ«'

# --------------------------------------------------------------------------
# --- Google Service Initialization (èªè¨¼å‡¦ç†) ---
# --------------------------------------------------------------------------

class DummyGSClient:
    """èªè¨¼å¤±æ•—æ™‚ç”¨ã®ãƒ€ãƒŸãƒ¼gspreadã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    def open(self, name): return self
    def worksheet(self, name): return self
    def get_all_records(self): return []
    def get_all_values(self): return []
    def append_row(self, values): pass
    
class DummyStorageClient:
    """èªè¨¼å¤±æ•—æ™‚ç”¨ã®ãƒ€ãƒŸãƒ¼GCSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    def bucket(self, name): return self
    def blob(self, name): return self
    def download_as_bytes(self): return b''
    def upload_from_file(self, file_obj, content_type): pass
    def get_bucket(self, name): return self
    def list_blobs(self, **kwargs): return []

# gc ã¨ storage_client ã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§å®šç¾©ï¼ˆDummyã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§åˆæœŸåŒ–ï¼‰
gc = DummyGSClient()
storage_client = DummyStorageClient()

@st.cache_resource(ttl=3600)
def initialize_google_services():
    """Streamlit Secretsã‹ã‚‰èªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ã€Googleã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–ã™ã‚‹"""
    
    if 'storage' not in globals():
        st.error("âŒ è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼: `google.cloud.storage` ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚Streamlitã®ç’°å¢ƒä¾å­˜ã¨æ€ã‚ã‚Œã¾ã™ã€‚")
        return DummyGSClient(), DummyStorageClient()
        
    if "gcs_credentials" not in st.secrets:
        st.error("âŒ è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼: Streamlit Cloudã®Secretsã« `gcs_credentials` ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return DummyGSClient(), DummyStorageClient()

    try:
        raw_credentials_string = st.secrets["gcs_credentials"]
        
        # --- èªè¨¼æ–‡å­—åˆ—ã®ã€å¼·åˆ¶ã€‘ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— v20.3.0 ---
        cleaned_string = raw_credentials_string.strip()
        cleaned_string = cleaned_string.replace('\n', '')
        cleaned_string = cleaned_string.replace('\t', '')
        cleaned_string = cleaned_string.replace('Â ', '') # U+00A0: NO-BREAK SPACE
        cleaned_string = re.sub(r'(\s){2,}', r'\1', cleaned_string)
        
        # JSONã‚’ãƒ‘ãƒ¼ã‚¹
        info = json.loads(cleaned_string) 
        
        # gspread (Spreadsheet) ã®èªè¨¼
        gc_real = gspread.service_account_from_dict(info)

        # google.cloud.storage (GCS) ã®èªè¨¼
        storage_client_real = storage.Client.from_service_account_info(info)

        st.sidebar.success("âœ… Googleã‚µãƒ¼ãƒ“ã‚¹èªè¨¼æˆåŠŸ")
        return gc_real, storage_client_real

    except json.JSONDecodeError as e:
        st.error(f"âŒ èªè¨¼ã‚¨ãƒ©ãƒ¼ï¼ˆJSONå½¢å¼ä¸æ­£ï¼‰: ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®JSONå½¢å¼ãŒä¸æ­£ã§ã™ã€‚ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
        return DummyGSClient(), DummyStorageClient()
        
    except Exception as e:
        st.error(f"âŒ èªè¨¼ã‚¨ãƒ©ãƒ¼: ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚èªè¨¼æƒ…å ±ã‚’ã”ç¢ºèªãã ã•ã„ã€‚({e})")
        return DummyGSClient(), DummyStorageClient()

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’åˆæœŸåŒ–ã•ã‚ŒãŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«æ›´æ–°
gc, storage_client = initialize_google_services() 

# --------------------------------------------------------------------------
# --- Data Utilities (ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»è§£æ) ---
# --------------------------------------------------------------------------

@st.cache_data(ttl=600, show_spinner="ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
def get_sheet_as_df(spreadsheet_name, sheet_name):
    """æŒ‡å®šã•ã‚ŒãŸã‚·ãƒ¼ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã¨ã—ã¦å–å¾—ã™ã‚‹"""
    global gc
    
    if isinstance(gc, DummyGSClient):
        st.warning("âš ï¸ èªè¨¼ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã—ã¾ã™ã€‚")
        return pd.DataFrame()
    
    try:
        worksheet = gc.open(spreadsheet_name).worksheet(sheet_name)
        data = worksheet.get_all_values()
        
        if not data or len(data) <= 1: 
            # ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã€ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã ã‘ã‚’åŸºã«ç©ºã®DataFrameã‚’ä½œæˆ
            return pd.DataFrame(columns=data[0] if data else [])
        
        df = pd.DataFrame(data[1:], columns=data[0])
        return df

    except gspread.exceptions.WorksheetNotFound:
        st.error(f"âŒ ã‚·ãƒ¼ãƒˆåã€Œ{sheet_name}ã€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
        return pd.DataFrame()
    except Exception as e:
        st.error(f"âŒ ã‚·ãƒ¼ãƒˆã€Œ{sheet_name}ã€ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚({e})")
        return pd.DataFrame()

# --- IV/PLãƒ‡ãƒ¼ã‚¿è§£æç”¨ã‚³ã‚¢ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ (PLè§£æã®ãŸã‚ã«ç¶­æŒ) ---
def _load_two_column_data_core(uploaded_file_bytes, column_names):
    """IV/PLãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰2åˆ—ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ©ãƒ åã‚’ä»˜ã‘ã¦DataFrameã‚’è¿”ã™"""
    try:
        # ãƒ­ãƒã‚¹ãƒˆãªèª­ã¿è¾¼ã¿ãƒ­ã‚¸ãƒƒã‚¯ 
        # (utf-8ãƒ‡ã‚³ãƒ¼ãƒ‰æ¸ˆã¿ã§æ¸¡ã•ã‚Œã‚‹å‰æã ã£ãŸãŒã€load_pl_dataãŒgetvalue()ã‚’æ¸¡ã™ãŸã‚ä¿®æ­£ãªã—)
        content = uploaded_file_bytes.decode('utf-8').splitlines()
        data_lines = content[1:] # 1è¡Œç›®ã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦ã‚¹ã‚­ãƒƒãƒ—

        cleaned_data_lines = []
        for line in data_lines:
            line_stripped = line.strip()
            if line_stripped and not line_stripped.startswith(('#', '!', '/')):
                cleaned_data_lines.append(line_stripped)

        if not cleaned_data_lines: return None

        data_string_io = io.StringIO("\n".join(cleaned_data_lines))
        
        # è¤‡æ•°ã®åŒºåˆ‡ã‚Šæ–‡å­—ã‚’è©¦ã™ãƒ­ãƒã‚¹ãƒˆãªèª­ã¿è¾¼ã¿
        try:
            df = pd.read_csv(data_string_io, sep=r'\s+', engine='python', header=None, skipinitialspace=True)
        except Exception:
            try:
                data_string_io.seek(0)
                df = pd.read_csv(data_string_io, sep='\t', engine='c', header=None)
            except Exception:
                data_string_io.seek(0)
                df = pd.read_csv(data_string_io, sep=',', engine='python', header=None)

        if df is None or len(df.columns) < 2: return None
        
        df = df.iloc[:, :2]
        df.columns = column_names # æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ©ãƒ åã‚’ä½¿ç”¨

        df[column_names[0]] = pd.to_numeric(df[column_names[0]], errors='coerce', downcast='float')
        df[column_names[1]] = pd.to_numeric(df[column_names[1]], errors='coerce', downcast='float')
        df.dropna(inplace=True)
        
        return df

    except Exception:
        return None

# --- IVãƒ‡ãƒ¼ã‚¿è§£æç”¨ (å®‰å®šç‰ˆã«ç½®ãæ›ãˆ) ---
@st.cache_data(show_spinner="IVãƒ‡ãƒ¼ã‚¿ã‚’è§£æä¸­...", max_entries=50)
def load_iv_data(uploaded_file):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸIVãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆTXT/CSVï¼‰ã‚’ãƒ­ãƒã‚¹ãƒˆã«èª­ã¿è¾¼ã‚€é–¢æ•°ã€‚"""
    
    file_name = uploaded_file.name
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚¤ãƒŠãƒªã¨ã—ã¦èª­ã¿è¾¼ã¿ã€æ–‡å­—åˆ—ã«ãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆUTF-8, Shift-JISã‚’è©¦è¡Œï¼‰
    try:
        data_string = uploaded_file.getvalue().decode('utf-8')
    except UnicodeDecodeError:
        try:
            data_string = uploaded_file.getvalue().decode('shift_jis')
        except:
            return None, file_name

    try:
        data_io = io.StringIO(data_string)
        
        # skiprows=1ã§æœ€åˆã®ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€ã‚¿ãƒ–/ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§èª­ã¿è¾¼ã‚€
        df = pd.read_csv(data_io, sep=r'\s+', skiprows=1, header=None, names=['VF(V)', 'IF(A)'])
        
        # ãƒ‡ãƒ¼ã‚¿å‹ã‚’æ•°å€¤ã«å¤‰æ›ï¼ˆã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹è¡Œã¯ç„¡è¦–ï¼‰
        df['VF(V)'] = pd.to_numeric(df['VF(V)'], errors='coerce')
        df['IF(A)'] = pd.to_numeric(df['IF(A)'], errors='coerce')
        df.dropna(inplace=True)

        return df, file_name

    except Exception:
        return None, file_name


# --- PLãƒ‡ãƒ¼ã‚¿è§£æç”¨ (å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’ç¶­æŒ) ---
@st.cache_data(show_spinner="PLãƒ‡ãƒ¼ã‚¿ã‚’è§£æä¸­...", max_entries=50)
def load_pl_data(uploaded_file):
    """PLãƒ•ã‚¡ã‚¤ãƒ« (pixel vs intensity) ã‚’èª­ã¿è¾¼ã¿ã€DataFrame (pixel, intensity) ã‚’è¿”ã™"""
    df = _load_two_column_data_core(uploaded_file.getvalue(), ['pixel', 'intensity'])
    # load_pl_dataã¯ã€uploaded_fileã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç›´æ¥å—ã‘å–ã‚‹ãŸã‚ã€getvalue()ã‚’ä½¿ç”¨
    if df is not None and not df.empty:
        return df[['pixel', 'intensity']]
    return None

# (å…ƒã®ã‚³ãƒ¼ãƒ‰ã«ã‚ã£ãŸ combine_dataframes ã¯ã€æ–°ã—ã„ IV ãƒ­ã‚¸ãƒƒã‚¯ã§ä¸è¦ã«ãªã£ãŸãŸã‚å‰Šé™¤)


# --------------------------------------------------------------------------
# --- GCS Utilities (ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰) ---
# --------------------------------------------------------------------------

def upload_file_to_gcs(storage_client, file_obj, folder_name):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€å…¬é–‹URLã‚’è¿”ã™"""
    if isinstance(storage_client, DummyStorageClient):
        return None, "dummy_url_gcs_error"
    timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    original_filename = file_obj.name
    safe_filename = original_filename.replace(' ', '_').replace('/', '_')
    gcs_filename = f"{folder_name}/{timestamp}_{safe_filename}"
    try:
        bucket = storage_client.bucket(CLOUD_STORAGE_BUCKET_NAME)
        blob = bucket.blob(gcs_filename)
        file_obj.seek(0)
        blob.upload_from_file(file_obj, content_type=file_obj.type)
        # Google Cloud Storageã®å…¬é–‹URLå½¢å¼
        public_url = f"https://storage.googleapis.com/{CLOUD_STORAGE_BUCKET_NAME}/{url_quote(gcs_filename)}"
        return original_filename, public_url
    except Exception as e:
        st.error(f"âŒ GCSã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚({e})")
        return None, None

# --------------------------------------------------------------------------
# --- Page Implementations (å„æ©Ÿèƒ½ãƒšãƒ¼ã‚¸) ---
# --------------------------------------------------------------------------

# --- æ±ç”¨çš„ãªä¸€è¦§è¡¨ç¤ºé–¢æ•° (å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’ç¶­æŒ) ---
def page_data_list(sheet_name, title, col_time, col_filter=None, col_memo=None, col_url=None, detail_cols=None):
    """æ±ç”¨çš„ãªãƒ‡ãƒ¼ã‚¿ä¸€è¦§ãƒšãƒ¼ã‚¸ (R2, R3, R1å¯¾å¿œ)"""
    st.header(f"ğŸ“š {title}ä¸€è¦§")
    df = get_sheet_as_df(SPREADSHEET_NAME, sheet_name)
    if df.empty:
        st.info("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    # ... (å…ƒã®çµã‚Šè¾¼ã¿ãƒ»æ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶­æŒ) ...
    st.subheader("çµã‚Šè¾¼ã¿ã¨æ¤œç´¢")
    
    # ã‚«ãƒ†ã‚´ãƒªãƒ»ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã«ã‚ˆã‚‹çµã‚Šè¾¼ã¿
    if col_filter and col_filter in df.columns: 
        # ç©ºç™½ãƒ‡ãƒ¼ã‚¿ã‚’ 'ãªã—' ã¨ã—ã¦æ‰±ã†
        df[col_filter] = df[col_filter].fillna('ãªã—')
        filter_options = ["ã™ã¹ã¦"] + sorted(list(df[col_filter].unique()))
        data_filter = st.selectbox(f"ã€Œ{col_filter}ã€ã§çµã‚Šè¾¼ã¿", filter_options)
        if data_filter != "ã™ã¹ã¦":
            df = df[df[col_filter] == data_filter]

    # æ—¥ä»˜ã«ã‚ˆã‚‹çµã‚Šè¾¼ã¿ (R2: é–‹å§‹æ—¥ã‚’2025/4/1ã«å›ºå®š)
    if col_time and col_time in df.columns:
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ—ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã¨æ—¥ä»˜å‹ã¸ã®å¤‰æ›
        try:
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å½¢å¼ ('YYYYMMDD_HHMMSS' ã¾ãŸã¯ 'YYYYMMDDHHMMSS') ã‹ã‚‰æ—¥ä»˜éƒ¨åˆ†ã®ã¿ã‚’å–å¾—
            df['date_only'] = pd.to_datetime(
                df[col_time].astype(str).str.replace(r'[^0-9]', '', regex=True).str[:8],
                errors='coerce', format='%Y%m%d'
            ).dt.date
        except:
            st.warning("âš ï¸ ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ—ã®å½¢å¼ãŒä¸æ­£ã§ã™ã€‚æ—¥ä»˜ã«ã‚ˆã‚‹çµã‚Šè¾¼ã¿ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
            df['date_only'] = pd.NaT # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ã‚’ç„¡åŠ¹åŒ–
        
        df_valid_date = df.dropna(subset=['date_only'])
        
        if not df_valid_date.empty:
            min_date = df_valid_date['date_only'].min()
            max_date = df_valid_date['date_only'].max()
            
            # R2: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–‹å§‹æ—¥ã‚’2025å¹´4æœˆ1æ—¥ã«è¨­å®š
            try:
                default_start_date = date(2025, 4, 1)
                if default_start_date < min_date:
                    default_start_date = min_date
            except ValueError:
                default_start_date = min_date

            date_range = st.date_input(
                "æ—¥ä»˜ç¯„å›²ã§çµã‚Šè¾¼ã¿", 
                value=(default_start_date, max_date), 
                min_value=min_date, 
                max_value=max_date
            )
            
            if len(date_range) == 2:
                start_date, end_date = date_range
                df = df[ (df['date_only'] >= start_date) & (df['date_only'] <= end_date) ]
            elif len(date_range) == 1:
                start_date = date_range[0]
                df = df[ df['date_only'] >= start_date ]

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
    search_query = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ (ãƒ¡ãƒ¢/ã‚¿ã‚¤ãƒˆãƒ«ãªã©)", value="")
    if search_query:
        df_search = pd.DataFrame()
        cols_to_search = [c for c in df.columns if c in [col_memo, HANDOVER_COL_TITLE, QA_COL_TITLE, QA_COL_CONTENT]]
        
        for col in cols_to_search:
            # æ¤œç´¢å¯¾è±¡åˆ—ãŒæ–‡å­—åˆ—å‹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
            if pd.api.types.is_object_dtype(df[col]):
                df_search = pd.concat([df_search, df[df[col].astype(str).str.contains(search_query, case=False, na=False)]]).drop_duplicates()
        
        df = df_search.sort_values(by=col_time, ascending=False)
    else:
        df = df.sort_values(by=col_time, ascending=False)

    st.subheader(f"æ¤œç´¢çµæœ ({len(df)}ä»¶)")

    # æœ€çµ‚çš„ãªè¡¨ç¤º (è©³ç´°åˆ—ã®è¡¨ç¤ºè¨­å®š)
    display_cols = [col_time]
    if col_filter: display_cols.append(col_filter)
    if detail_cols: display_cols.extend(detail_cols)

    # DataFrameè¡¨ç¤º
    st.dataframe(df[display_cols].reset_index(drop=True), use_container_width=True)


def page_epi_note():
    # ... (å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’ç¶­æŒ) ...
    st.header("ğŸ“ ã‚¨ãƒ”ãƒãƒ¼ãƒˆè¨˜éŒ²")
    st.markdown("æˆé•·ã‚„å®Ÿé¨“ã®è¨˜éŒ²ã‚’å…¥åŠ›ã—ã€æŒ‡å®šã®Google SpreadSheetã«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã—ã¾ã™ã€‚")
    
    # ... (å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶­æŒ) ...
    NOTE_TYPE = 'ã‚¨ãƒ”ãƒãƒ¼ãƒˆ'
    FOLDER_NAME = 'epi_files'
    
    col1, col2 = st.columns(2)
    with col1:
        category = st.selectbox("ã‚«ãƒ†ã‚´ãƒª (è£…ç½®/ãƒ†ãƒ¼ãƒ)", ["D1", "D2", "MBE", "RTA", "ALD", "ãã®ä»–"], key='epi_category')
    with col2:
        file_attachments = st.file_uploader("å†™çœŸ/ãƒ•ã‚¡ã‚¤ãƒ«æ·»ä»˜", type=['jpg', 'png', 'pdf'], accept_multiple_files=True, key='epi_attachments')

    memo = st.text_area("ãƒ¡ãƒ¢ (è¨˜éŒ²å†…å®¹)", height=300, key='epi_memo')
    
    if st.button("è¨˜éŒ²ã‚’ä¿å­˜ (ã‚¨ãƒ”ãƒãƒ¼ãƒˆ)", key='save_epi_note_button'):
        if not memo:
            st.error("ãƒ¡ãƒ¢ï¼ˆè¨˜éŒ²å†…å®¹ï¼‰ã¯å¿…é ˆã§ã™ã€‚")
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        filenames_json = json.dumps([f.name for f in file_attachments])
        urls_list = []
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†
        with st.spinner("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Cloud Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­..."):
            for uploaded_file in file_attachments:
                original_filename, public_url = upload_file_to_gcs(storage_client, uploaded_file, FOLDER_NAME)
                if public_url:
                    urls_list.append(public_url)

        urls_json = json.dumps(urls_list)
        
        row_data = [
            timestamp, NOTE_TYPE, category, memo, filenames_json, urls_json # JSONæ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜
        ]
        
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿
        try:
            gc.open(SPREADSHEET_NAME).worksheet(SHEET_EPI_DATA).append_row(row_data)
            st.success("ã‚¨ãƒ”ãƒãƒ¼ãƒˆã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã—ã¾ã—ãŸã€‚")
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¦å†èª­ã¿è¾¼ã¿
            st.cache_data.clear(); st.rerun()
        except Exception as e:
            st.error(f"ãƒ‡ãƒ¼ã‚¿ã®æ›¸ãè¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚·ãƒ¼ãƒˆå '{SHEET_EPI_DATA}' ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            st.exception(e)

def page_mainte_note():
    # ... (å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’ç¶­æŒ) ...
    st.header("ğŸ› ï¸ ãƒ¡ãƒ³ãƒ†ãƒãƒ¼ãƒˆè¨˜éŒ²")
    st.markdown("è£…ç½®ã®ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹è¨˜éŒ²ã‚’å…¥åŠ›ã—ã€æŒ‡å®šã®Google SpreadSheetã«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã—ã¾ã™ã€‚")
    
    # ... (å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶­æŒ) ...
    NOTE_TYPE = 'ãƒ¡ãƒ³ãƒ†ãƒãƒ¼ãƒˆ'
    FOLDER_NAME = 'mainte_files'
    
    memo = st.text_area("ãƒ¡ãƒ¢ (è¨˜éŒ²å†…å®¹/ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å®Ÿæ–½æ—¥ã¨å†…å®¹)", height=200, key='mainte_memo')
    file_attachments = st.file_uploader("å†™çœŸ/ãƒ•ã‚¡ã‚¤ãƒ«æ·»ä»˜", type=['jpg', 'png', 'pdf'], accept_multiple_files=True, key='mainte_attachments')

    if st.button("è¨˜éŒ²ã‚’ä¿å­˜ (ãƒ¡ãƒ³ãƒ†ãƒãƒ¼ãƒˆ)", key='save_mainte_note_button'):
        if not memo:
            st.error("ãƒ¡ãƒ¢ï¼ˆè¨˜éŒ²å†…å®¹ï¼‰ã¯å¿…é ˆã§ã™ã€‚")
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        filenames_json = json.dumps([f.name for f in file_attachments])
        urls_list = []
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†
        with st.spinner("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Cloud Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­..."):
            for uploaded_file in file_attachments:
                original_filename, public_url = upload_file_to_gcs(storage_client, uploaded_file, FOLDER_NAME)
                if public_url:
                    urls_list.append(public_url)

        urls_json = json.dumps(urls_list)
        
        row_data = [
            timestamp, NOTE_TYPE, memo, filenames_json, urls_json # JSONæ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜
        ]
        
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿
        try:
            gc.open(SPREADSHEET_NAME).worksheet(SHEET_MAINTE_DATA).append_row(row_data)
            st.success("ãƒ¡ãƒ³ãƒ†ãƒãƒ¼ãƒˆã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã—ã¾ã—ãŸã€‚")
            st.cache_data.clear(); st.rerun()
        except Exception as e:
            st.error(f"ãƒ‡ãƒ¼ã‚¿ã®æ›¸ãè¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚·ãƒ¼ãƒˆå '{SHEET_MAINTE_DATA}' ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            st.exception(e)


def page_epi_note_list():
    page_data_list(SHEET_EPI_DATA, "ã‚¨ãƒ”ãƒãƒ¼ãƒˆ", EPI_COL_TIMESTAMP, col_filter=EPI_COL_CATEGORY, detail_cols=[EPI_COL_MEMO, EPI_COL_FILENAME, EPI_COL_FILE_URL])

def page_mainte_note_list():
    page_data_list(SHEET_MAINTE_DATA, "ãƒ¡ãƒ³ãƒ†ãƒãƒ¼ãƒˆ", MAINT_COL_TIMESTAMP, detail_cols=[MAINT_COL_MEMO, MAINT_COL_FILENAME, MAINT_COL_FILE_URL])

def page_meeting_note():
    # ... (å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’ç¶­æŒ) ...
    st.header("ğŸ“‹ è­°äº‹éŒ²ç®¡ç†")
    st.markdown("è­°äº‹éŒ²ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€Google SpreadSheetã«è¨˜éŒ²ã—ã¾ã™ã€‚")
    
    # ... (å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶­æŒ) ...
    MEETING_FOLDER_NAME = 'meeting_audio'
    
    meeting_title = st.text_input("ä¼šè­°ã‚¿ã‚¤ãƒˆãƒ«/æ—¥ä»˜", key='meeting_title')
    audio_file = st.file_uploader("ä¼šè­°ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ« (.m4a, .mp3ãªã©)", type=['m4a', 'mp3', 'wav'], key='audio_file')
    content = st.text_area("è­°äº‹éŒ²å†…å®¹ (ã¾ãŸã¯æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆ)", height=300, key='meeting_content')
    
    if st.button("è­°äº‹éŒ²ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–", key='archive_meeting_button'):
        if not meeting_title or not content:
            st.error("ä¼šè­°ã‚¿ã‚¤ãƒˆãƒ«ã¨è­°äº‹éŒ²å†…å®¹ã¯å¿…é ˆã§ã™ã€‚")
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        audio_filename = ""
        audio_url = ""
        
        if audio_file:
            with st.spinner("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Cloud Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                audio_filename, audio_url = upload_file_to_gcs(storage_client, audio_file, MEETING_FOLDER_NAME)

        row_data = [
            timestamp, meeting_title, audio_filename, audio_url, content
        ]
        
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿
        try:
            gc.open(SPREADSHEET_NAME).worksheet(SHEET_MEETING_DATA).append_row(row_data)
            st.success("è­°äº‹éŒ²ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã—ã¾ã—ãŸã€‚")
            st.cache_data.clear(); st.rerun()
        except Exception as e:
            st.error(f"ãƒ‡ãƒ¼ã‚¿ã®æ›¸ãè¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚·ãƒ¼ãƒˆå '{SHEET_MEETING_DATA}' ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            st.exception(e)

def page_meeting_note_list():
    page_data_list(SHEET_MEETING_DATA, "è­°äº‹éŒ²", MEETING_COL_TIMESTAMP, detail_cols=[MEETING_COL_TITLE, MEETING_COL_AUDIO_NAME, MEETING_COL_AUDIO_URL, MEETING_COL_CONTENT])


# --------------------------------------------------------------------------
# --- Page Implementations: IVãƒ‡ãƒ¼ã‚¿è§£æ (å®‰å®šå‹•ä½œç‰ˆ) ---
# --------------------------------------------------------------------------
# **[REPLACED] Stable page_iv_analysis (fixes UnboundLocalError and 10-file limit)**
def page_iv_analysis():
    st.header("âš¡ IV Data Analysis (IVãƒ‡ãƒ¼ã‚¿è§£æ)")
    st.markdown(f"IVãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã€ã‚°ãƒ©ãƒ•æç”»ã¨ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’è¡Œã„ã¾ã™ã€‚**ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒ{MAX_COMBINED_FILES}å€‹ä»¥ä¸‹ã®å ´åˆã€çµåˆãƒ‡ãƒ¼ã‚¿ã‚‚ä½œæˆã—ã¾ã™ã€‚**")

    uploaded_files = st.file_uploader(
        "IVãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.txt ã¾ãŸã¯ .csvï¼‰ã‚’é¸æŠã—ã¦ãã ã•ã„",
        type=['txt', 'csv'],
        accept_multiple_files=True
    )

    if uploaded_files:
        st.subheader("ğŸ“Š IV Characteristic Plot")
        
        # ã‚°ãƒ©ãƒ•ã‚µã‚¤ã‚ºã‚’å¤§ãã
        fig, ax = plt.subplots(figsize=(12, 7))
        
        all_data_for_export = [] # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®DFã¨ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ ¼ç´
        
        # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨ã‚°ãƒ©ãƒ•æç”»
        for uploaded_file in uploaded_files:
            # æ–°ã—ã„å®‰å®šç‰ˆã®ãƒ­ãƒ¼ãƒ‰é–¢æ•°ã‚’ä½¿ç”¨
            df, file_name = load_iv_data(uploaded_file) 
            
            if df is not None and not df.empty:
                voltage_col = 'VF(V)'
                current_col = 'IF(A)'
                
                # ã‚°ãƒ©ãƒ•ã«ãƒ—ãƒ­ãƒƒãƒˆ
                ax.plot(df[voltage_col], df[current_col], label=file_name)
                
                # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆç”¨ã«[Voltage_V, Current_A_filename]ã®DFã‚’ä½œæˆ
                df_export = df.rename(columns={voltage_col: 'Voltage_V', current_col: f'Current_A_{file_name}'})
                all_data_for_export.append({'name': file_name, 'df': df_export})

        
        # ã‚°ãƒ©ãƒ•è¨­å®š (æ–‡å­—åŒ–ã‘å¯¾ç­–: ã™ã¹ã¦è‹±èª)
        ax.set_title('IV Characteristic Plot', fontsize=16)
        ax.set_xlabel('Voltage (V)', fontsize=14)
        ax.set_ylabel('Current (A)', fontsize=14)
        ax.grid(True, linestyle='--', alpha=0.6)
        ax.legend(title='File Name', loc='best')
        ax.ticklabel_format(style='sci', axis='y', scilimits=(0, 0))
        
        st.pyplot(fig, use_container_width=True)
        plt.close(fig) # ãƒ¡ãƒ¢ãƒªè§£æ”¾

        # ------------------------------------------------------------------
        # 2. Excelã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ (æ¡ä»¶åˆ†å²ãƒ­ã‚¸ãƒƒã‚¯)
        # ------------------------------------------------------------------
        if all_data_for_export:
            st.subheader("ğŸ“ ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
            
            output = BytesIO()
            file_count = len(all_data_for_export)
            
            # 10å€‹ä»¥ä¸‹ã®å ´åˆã¯çµåˆãƒ•ãƒ©ã‚°ã‚’Trueã«
            SHOULD_COMBINE = file_count <= MAX_COMBINED_FILES
            
            if SHOULD_COMBINE:
                st.info(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒ{file_count}å€‹ã®ãŸã‚ã€å€‹åˆ¥ã‚·ãƒ¼ãƒˆã«åŠ ãˆã¦**çµåˆãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆ**ã‚’ä½œæˆã—ã¾ã™ã€‚")
            else:
                st.warning(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒ{file_count}å€‹ã¨å¤šã„ãŸã‚ã€ã‚¯ãƒ©ãƒƒã‚·ãƒ¥é˜²æ­¢ã®ãŸã‚**å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆã®ã¿**ã‚’ä½œæˆã—ã¾ã™ã€‚ï¼ˆçµåˆã‚·ãƒ¼ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ï¼‰")
            
            with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’Excelã«æ›¸ãè¾¼ã‚“ã§ã„ã¾ã™..."):
                with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                    
                    # --- (A) å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ¥ã‚·ãƒ¼ãƒˆã«å‡ºåŠ› (å…±é€šå‡¦ç†) ---
                    for data_item in all_data_for_export:
                        file_name = data_item['name']
                        df_export = data_item['df']
                        
                        sheet_name = file_name.replace('.txt', '').replace('.csv', '')
                        # Excelã®ã‚·ãƒ¼ãƒˆååˆ¶é™(31æ–‡å­—)
                        if len(sheet_name) > 31:
                            sheet_name = sheet_name[:28] 
                        
                        df_export.to_excel(writer, sheet_name=sheet_name, index=False)
                        
                        # å€‹åˆ¥DFã®ãƒ¡ãƒ¢ãƒªã‚’ç›´å¾Œã«è§£æ”¾
                        del df_export

                    # --- (B) çµåˆãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ› (10å€‹ä»¥ä¸‹ã®å ´åˆã®ã¿) ---
                    if SHOULD_COMBINE:
                        
                        # æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’åŸºæº–ã«çµåˆã‚’é–‹å§‹
                        start_df = all_data_for_export[0]['df']
                        combined_df = start_df.copy() 
                        
                        # 2ã¤ç›®ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ 'Voltage_V' ã‚’ã‚­ãƒ¼ã«çµåˆ
                        for i in range(1, len(all_data_for_export)):
                            item = all_data_for_export[i]
                            df_current = item['df']
                            # 'Voltage_V'ã‚’ã‚­ãƒ¼ã«ã€2ã¤ç›®ã®åˆ—ï¼ˆé›»æµãƒ‡ãƒ¼ã‚¿ï¼‰ã®ã¿ã‚’çµåˆ
                            combined_df = pd.merge(combined_df, df_current[['Voltage_V', df_current.columns[1]]], on='Voltage_V', how='outer')
                        
                        # é›»åœ§é †ã«ã‚½ãƒ¼ãƒˆ
                        combined_df.sort_values(by='Voltage_V', inplace=True)
                        
                        # çµåˆDFã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
                        st.dataframe(combined_df.head())
                        
                        # çµåˆDFã‚’æœ€çµ‚ã‚·ãƒ¼ãƒˆã«å‡ºåŠ›
                        combined_df.to_excel(writer, sheet_name='__COMBINED_DATA__', index=False)
                        
                        # å‡¦ç†è½ã¡å¯¾ç­–: çµåˆDFã®ãƒ¡ãƒ¢ãƒªã‚’ç›´å¾Œã«è§£æ”¾
                        del combined_df
                        
            
            processed_data = output.getvalue()
            
            download_label = "ğŸ“ˆ çµåˆ/å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€Excelãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰" if SHOULD_COMBINE else "ğŸ“ å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å€‹åˆ¥ã‚·ãƒ¼ãƒˆã«ä¿å­˜ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
            
            st.download_button(
                label=download_label,
                data=processed_data,
                file_name=f"iv_analysis_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
            
        else:
            st.warning("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# --------------------------------------------------------------------------
# --- Page Implementations: PLãƒ‡ãƒ¼ã‚¿è§£æ (å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’ç¶­æŒ) ---
# --------------------------------------------------------------------------

def page_pl_analysis():
    # ... (å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’ç¶­æŒ) ...
    st.header("ğŸ”¬ PL Data Analysis (PLãƒ‡ãƒ¼ã‚¿è§£æ)")
    st.markdown("PLãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã€ã‚°ãƒ©ãƒ•æç”»ã¨ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’è¡Œã„ã¾ã™ã€‚")
    
    # Session Stateã®åˆæœŸåŒ– (æ³¢é•·æ ¡æ­£ä¿‚æ•°)
    if 'pl_calib_a' not in st.session_state:
        st.session_state.pl_calib_a = 0.0
    if 'pl_calib_b' not in st.session_state:
        st.session_state.pl_calib_b = 0.0

    uploaded_files = st.file_uploader(
        "PLãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.txt ã¾ãŸã¯ .csvï¼‰ã‚’é¸æŠã—ã¦ãã ã•ã„",
        type=['txt', 'csv'],
        accept_multiple_files=True,
        key='pl_files'
    )

    if uploaded_files:
        st.subheader("æ³¢é•·æ ¡æ­£ (Wavelength Calibration)")
        
        col_calib_a, col_calib_b = st.columns(2)
        with col_calib_a:
            st.session_state.pl_calib_a = st.number_input(
                "æ ¡æ­£ä¿‚æ•° a (Wavelength = a * pixel + b)",
                value=st.session_state.pl_calib_a,
                format="%.6f",
                key='pl_input_a'
            )
        with col_calib_b:
            st.session_state.pl_calib_b = st.number_input(
                "æ ¡æ­£ä¿‚æ•° b (Wavelength = a * pixel + b)",
                value=st.session_state.pl_calib_b,
                format="%.6f",
                key='pl_input_b'
            )
        
        a = st.session_state.pl_calib_a
        b = st.session_state.pl_calib_b

        st.subheader("ğŸ“Š PL Characteristic Plot")
        
        fig, ax = plt.subplots(figsize=(12, 7))
        all_data_for_export = []
        
        # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨ã‚°ãƒ©ãƒ•æç”»
        for uploaded_file in uploaded_files:
            df = load_pl_data(uploaded_file)
            file_name = uploaded_file.name
            
            if df is not None and not df.empty:
                # ãƒ”ã‚¯ã‚»ãƒ«ã‚’æ³¢é•·ã«å¤‰æ›
                df['wavelength_nm'] = df['pixel'] * a + b
                
                # ã‚°ãƒ©ãƒ•ã«ãƒ—ãƒ­ãƒƒãƒˆ
                ax.plot(df['wavelength_nm'], df['intensity'], label=file_name)
                
                # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆç”¨ã«åˆ—åã‚’æ•´å½¢
                df_export = df.rename(columns={'wavelength_nm': 'Wavelength_nm', 'intensity': f'Intensity_{file_name}'})
                all_data_for_export.append({'name': file_name, 'df': df_export[['Wavelength_nm', f'Intensity_{file_name}']]})

        
        # ã‚°ãƒ©ãƒ•è¨­å®š (æ–‡å­—åŒ–ã‘å¯¾ç­–: ã™ã¹ã¦è‹±èª)
        ax.set_title('PL Spectrum Plot', fontsize=16)
        ax.set_xlabel('Wavelength (nm)', fontsize=14)
        ax.set_ylabel('PL Intensity (a.u.)', fontsize=14)
        ax.grid(True, linestyle='--', alpha=0.6)
        ax.legend(title='File Name', loc='best')
        # è»¸ã®ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ä»»ã›ã‚‹
        
        st.pyplot(fig, use_container_width=True)
        plt.close(fig) # ãƒ¡ãƒ¢ãƒªè§£æ”¾

        # ------------------------------------------------------------------
        # 2. Excelã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ (IVè§£æã¨åŒæ§˜ã®çµåˆãƒ­ã‚¸ãƒƒã‚¯ã‚’PLã«é©ç”¨)
        # ------------------------------------------------------------------
        if all_data_for_export:
            st.subheader("ğŸ“ ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
            
            output = BytesIO()
            file_count = len(all_data_for_export)
            
            # PLãƒ‡ãƒ¼ã‚¿ã‚‚10å€‹ä»¥ä¸‹ã®å ´åˆã¯çµåˆãƒ•ãƒ©ã‚°ã‚’Trueã« (IVè§£æã‹ã‚‰æµç”¨)
            SHOULD_COMBINE = file_count <= MAX_COMBINED_FILES
            
            if SHOULD_COMBINE:
                st.info(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒ{file_count}å€‹ã®ãŸã‚ã€å€‹åˆ¥ã‚·ãƒ¼ãƒˆã«åŠ ãˆã¦**çµåˆãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆ**ã‚’ä½œæˆã—ã¾ã™ã€‚")
            else:
                st.warning(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒ{file_count}å€‹ã¨å¤šã„ãŸã‚ã€ã‚¯ãƒ©ãƒƒã‚·ãƒ¥é˜²æ­¢ã®ãŸã‚**å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆã®ã¿**ã‚’ä½œæˆã—ã¾ã™ã€‚ï¼ˆçµåˆã‚·ãƒ¼ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã™ï¼‰")
            
            with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’Excelã«æ›¸ãè¾¼ã‚“ã§ã„ã¾ã™..."):
                with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                    
                    # --- (A) å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ¥ã‚·ãƒ¼ãƒˆã«å‡ºåŠ› (å…±é€šå‡¦ç†) ---
                    for data_item in all_data_for_export:
                        file_name = data_item['name']
                        df_export = data_item['df']
                        
                        sheet_name = file_name.replace('.txt', '').replace('.csv', '')
                        if len(sheet_name) > 31:
                            sheet_name = sheet_name[:28] 
                        
                        df_export.to_excel(writer, sheet_name=sheet_name, index=False)
                        
                        # å€‹åˆ¥DFã®ãƒ¡ãƒ¢ãƒªã‚’ç›´å¾Œã«è§£æ”¾
                        del df_export

                    # --- (B) çµåˆãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ› (10å€‹ä»¥ä¸‹ã®å ´åˆã®ã¿) ---
                    if SHOULD_COMBINE:
                        
                        # æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’åŸºæº–ã«çµåˆã‚’é–‹å§‹
                        start_df = all_data_for_export[0]['df']
                        combined_df = start_df.copy() 
                        
                        # 2ã¤ç›®ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ 'Wavelength_nm' ã‚’ã‚­ãƒ¼ã«çµåˆ
                        for i in range(1, len(all_data_for_export)):
                            item = all_data_for_export[i]
                            df_current = item['df']
                            # 'Wavelength_nm'ã‚’ã‚­ãƒ¼ã«ã€2ã¤ç›®ã®åˆ—ï¼ˆå¼·åº¦ãƒ‡ãƒ¼ã‚¿ï¼‰ã®ã¿ã‚’çµåˆ
                            combined_df = pd.merge(combined_df, df_current[['Wavelength_nm', df_current.columns[1]]], on='Wavelength_nm', how='outer')
                        
                        # æ³¢é•·é †ã«ã‚½ãƒ¼ãƒˆ (æ˜‡é †)
                        combined_df.sort_values(by='Wavelength_nm', inplace=True)
                        
                        # çµåˆDFã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
                        st.dataframe(combined_df.head())
                        
                        # çµåˆDFã‚’æœ€çµ‚ã‚·ãƒ¼ãƒˆã«å‡ºåŠ›
                        combined_df.to_excel(writer, sheet_name='__COMBINED_DATA__', index=False)
                        
                        # å‡¦ç†è½ã¡å¯¾ç­–: çµåˆDFã®ãƒ¡ãƒ¢ãƒªã‚’ç›´å¾Œã«è§£æ”¾
                        del combined_df
                        
            
            processed_data = output.getvalue()
            
            download_label = "ğŸ“ˆ çµåˆ/å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€Excelãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰" if SHOULD_COMBINE else "ğŸ“ å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å€‹åˆ¥ã‚·ãƒ¼ãƒˆã«ä¿å­˜ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
            
            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã«ä¸­å¿ƒæ³¢é•·ã‚’ä»˜ã‘ã‚‹ (å…ƒã®ã‚³ãƒ¼ãƒ‰ã‹ã‚‰æµç”¨)
            center_wavelength_input = st.number_input("å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ç”¨ã™ã‚‹ä¸­å¿ƒæ³¢é•· (nm)", value=800, key='pl_center_wavelength_input')
            
            st.download_button(
                label=download_label,
                data=processed_data,
                file_name=f"pl_analysis_export_{center_wavelength_input}nm_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
            
        else:
            st.warning("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        st.info("æ¸¬å®šãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

# --- Dummy Pages (æœªå®Ÿè£…ã®ãƒšãƒ¼ã‚¸) ---
# ... (å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’ç¶­æŒ) ...
def page_calendar():
    st.header("ğŸ—“ï¸ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ»è£…ç½®äºˆç´„")
    st.info("ã“ã®ãƒšãƒ¼ã‚¸ã¯æœªå®Ÿè£…ã§ã™ã€‚")
    # ... (å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’ç¶­æŒ) ...

def page_qa_box():
    # ... (å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’ç¶­æŒ) ...
    st.header("ğŸ’¡ çŸ¥æµè¢‹ãƒ»è³ªå•ç®±")
    st.markdown("è³ªå•ã‚’æŠ•ç¨¿ã—ã€éå»ã®è³ªå•ãƒ»å›ç­”ã‚’é–²è¦§ã—ã¾ã™ã€‚")
    
    # ... (å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶­æŒ) ...
    QA_FOLDER_NAME = 'qa_files'

    # --- è³ªå•æŠ•ç¨¿ãƒ•ã‚©ãƒ¼ãƒ  ---
    with st.expander("â“ è³ªå•ã‚’æŠ•ç¨¿ã™ã‚‹"):
        title = st.text_input("è³ªå•ã‚¿ã‚¤ãƒˆãƒ«", key='qa_title')
        content = st.text_area("è³ªå•å†…å®¹", height=200, key='qa_content')
        contact = st.text_input("é€£çµ¡å…ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ (ä»»æ„)", key='qa_contact')
        file_attachments = st.file_uploader("æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«", accept_multiple_files=False, key='qa_attachments')
        
        if st.button("è³ªå•ã‚’æŠ•ç¨¿", key='post_qa_button'):
            if not title or not content:
                st.error("è³ªå•ã‚¿ã‚¤ãƒˆãƒ«ã¨è³ªå•å†…å®¹ã¯å¿…é ˆã§ã™ã€‚")
                return
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            filename = ""
            file_url = ""
            
            if file_attachments:
                with st.spinner("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Cloud Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                    filename, file_url = upload_file_to_gcs(storage_client, file_attachments, QA_FOLDER_NAME)

            row_data = [
                timestamp, title, content, contact, filename, file_url, "æœªè§£æ±º"
            ]
            
            # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿
            try:
                gc.open(SPREADSHEET_NAME).worksheet(SHEET_QA_DATA).append_row(row_data)
                st.success("è³ªå•ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã—ã¾ã—ãŸã€‚")
                st.cache_data.clear(); st.rerun()
            except Exception as e:
                st.error(f"ãƒ‡ãƒ¼ã‚¿ã®æ›¸ãè¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚·ãƒ¼ãƒˆå '{SHEET_QA_DATA}' ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                st.exception(e)

    # --- è³ªå•ã¨å›ç­”ã®ä¸€è¦§è¡¨ç¤º ---
    st.subheader("ğŸ“‹ éå»ã®è³ªå•ã¨å›ç­”")
    df_questions = get_sheet_as_df(SPREADSHEET_NAME, SHEET_QA_DATA)
    df_answers = get_sheet_as_df(SPREADSHEET_NAME, SHEET_QA_ANSWER)

    if not df_questions.empty:
        # è³ªå•IDã‚’ã‚­ãƒ¼ã«çµåˆ
        df_merged = pd.merge(
            df_questions, 
            df_answers[['è³ªå•ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ— (è³ªå•ID)', 'è§£ç­”å†…å®¹', 'è§£ç­”è€… (ä»»æ„)']],
            left_on=QA_COL_TIMESTAMP,
            right_on='è³ªå•ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ— (è³ªå•ID)',
            how='left'
        )
        
        # è¡¨ç¤ºç”¨ã®åˆ—ã‚’é¸æŠã—ã€æ–°ã—ã„åˆ—åã§æ•´ç†
        df_display = df_merged[[
            QA_COL_TIMESTAMP, QA_COL_TITLE, QA_COL_CONTENT, QA_COL_STATUS, QA_COL_FILE_URL,
            'è§£ç­”å†…å®¹', 'è§£ç­”è€… (ä»»æ„)'
        ]].rename(columns={
            QA_COL_TIMESTAMP: 'è³ªå•ID',
            QA_COL_TITLE: 'ã‚¿ã‚¤ãƒˆãƒ«',
            QA_COL_CONTENT: 'è³ªå•å†…å®¹',
            QA_COL_STATUS: 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹',
            QA_COL_FILE_URL: 'æ·»ä»˜URL',
            'è§£ç­”è€… (ä»»æ„)': 'è§£ç­”è€…'
        })
        
        # çµã‚Šè¾¼ã¿
        status_filter = st.selectbox("ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã§çµã‚Šè¾¼ã¿", ["ã™ã¹ã¦"] + list(df_display['ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹'].unique()), key='qa_status_filter')
        if status_filter != "ã™ã¹ã¦":
            df_display = df_display[df_display['ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹'] == status_filter]
            
        search_query = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ (ã‚¿ã‚¤ãƒˆãƒ«/å†…å®¹)", key='qa_search_query')
        if search_query:
             df_display = df_display[
                df_display['ã‚¿ã‚¤ãƒˆãƒ«'].astype(str).str.contains(search_query, case=False, na=False) |
                df_display['è³ªå•å†…å®¹'].astype(str).str.contains(search_query, case=False, na=False)
            ]
        
        st.dataframe(df_display.sort_values(by='è³ªå•ID', ascending=False).reset_index(drop=True), use_container_width=True)
    else:
        st.info("ã¾ã è³ªå•ã¯æŠ•ç¨¿ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

def page_handoff_notes():
    # ... (å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’ç¶­æŒ) ...
    st.header("ğŸ¤ è£…ç½®å¼•ãç¶™ããƒ¡ãƒ¢")
    st.markdown("è£…ç½®ã®ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‚„å¼•ãç¶™ãæƒ…å ±ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã—ã€ä¸€è¦§è¡¨ç¤ºã—ã¾ã™ã€‚")
    
    # ... (å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶­æŒ) ...
    HANDOVER_FOLDER_NAME = 'handoff_files'
    
    # --- è¨˜éŒ²ãƒ•ã‚©ãƒ¼ãƒ  ---
    with st.expander("ğŸ“ å¼•ãç¶™ãæƒ…å ±ã‚’è¨˜éŒ²ã™ã‚‹"):
        ho_type = st.selectbox("ç¨®é¡", ["ãƒãƒ‹ãƒ¥ã‚¢ãƒ«", "æ‰‹é †æ›¸", "ãã®ä»–ãƒ¡ãƒ¢"], key='ho_type')
        ho_title = st.text_input("ã‚¿ã‚¤ãƒˆãƒ«/è£…ç½®å", key='ho_title')
        ho_url = st.text_input("é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«/ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®URL (G Driveãªã©)", key='ho_url')
        ho_memo = st.text_area("ãƒ¡ãƒ¢", height=150, key='ho_memo')
        
        if st.button("è¨˜éŒ²ã‚’ä¿å­˜ (å¼•ãç¶™ã)", key='save_handoff_button'):
            if not ho_title or not ho_url:
                st.error("ã‚¿ã‚¤ãƒˆãƒ«ã¨URLã¯å¿…é ˆã§ã™ã€‚")
                return
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # å†…å®¹1,2,3ã®åˆ—ã¯ä½¿ã‚ãšã€ãƒ¡ãƒ¢åˆ—ã«çµ±åˆ
            row_data = [
                timestamp, ho_type, ho_title, ho_url, "", "", ho_memo 
            ]
            
            # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿
            try:
                # å…ƒã®ã‚³ãƒ¼ãƒ‰ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç¶­æŒã™ã‚‹ãŸã‚ã€å†…å®¹1-3ã‚‚ç©ºã§æ¸¡ã™
                gc.open(SPREADSHEET_NAME).worksheet(SHEET_HANDOVER_DATA).append_row(row_data)
                st.success("å¼•ãç¶™ãæƒ…å ±ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã—ã¾ã—ãŸã€‚")
                st.cache_data.clear(); st.rerun()
            except Exception as e:
                st.error(f"ãƒ‡ãƒ¼ã‚¿ã®æ›¸ãè¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚·ãƒ¼ãƒˆå '{SHEET_HANDOVER_DATA}' ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                st.exception(e)

    # --- ä¸€è¦§è¡¨ç¤º ---
    page_data_list(SHEET_HANDOVER_DATA, "è£…ç½®å¼•ãç¶™ããƒ¡ãƒ¢", HANDOVER_COL_TIMESTAMP, col_filter=HANDOVER_COL_TYPE, detail_cols=[HANDOVER_COL_TITLE, HANDOVER_COL_MEMO])


def page_trouble_report():
    # ... (å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’ç¶­æŒ) ...
    st.header("ğŸš¨ ãƒˆãƒ©ãƒ–ãƒ«å ±å‘Š")
    st.markdown("è£…ç½®ã®ãƒˆãƒ©ãƒ–ãƒ«å†…å®¹ã‚’å ±å‘Šãƒ»è¨˜éŒ²ã—ã€Google SpreadSheetã¨Cloud Storageã«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã—ã¾ã™ã€‚")
    
    # ... (å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶­æŒ) ...
    TROUBLE_FOLDER_NAME = 'trouble_files'
    
    # --- å ±å‘Šãƒ•ã‚©ãƒ¼ãƒ  ---
    with st.expander("ğŸ“ ãƒˆãƒ©ãƒ–ãƒ«ã‚’å ±å‘Šã™ã‚‹"):
        col_t1, col_t2 = st.columns(2)
        with col_t1:
            device = st.selectbox("æ©Ÿå™¨/å ´æ‰€", ["MBE", "RTA", "ALD", "D1", "D2", "ãã®ä»–"], key='trouble_device')
        with col_t2:
            report_date = st.date_input("ç™ºç”Ÿæ—¥", key='trouble_date')
            
        report_title = st.text_input("ä»¶å/ã‚¿ã‚¤ãƒˆãƒ«", key='trouble_title')
        t_occur = st.text_area("ãƒˆãƒ©ãƒ–ãƒ«ç™ºç”Ÿæ™‚ã®çŠ¶æ³ (ç™ºç”Ÿæ™‚é–“å«ã‚€)", height=150, key='trouble_occur')
        t_cause = st.text_area("åŸå› /ç©¶æ˜", height=150, key='trouble_cause')
        t_solution = st.text_area("å¯¾ç­–/å¾©æ—§å†…å®¹", height=150, key='trouble_solution')
        t_prevention = st.text_area("å†ç™ºé˜²æ­¢ç­–", height=150, key='trouble_prevention')
        reporter_name = st.text_input("å ±å‘Šè€…å", key='trouble_reporter')
        file_attachments = st.file_uploader("é–¢é€£å†™çœŸ/ãƒ•ã‚¡ã‚¤ãƒ«æ·»ä»˜", accept_multiple_files=True, key='trouble_attachments')
        
        if st.button("å ±å‘Šã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–", key='archive_trouble_button'):
            if not report_title or not t_occur or not reporter_name:
                st.error("ä»¶å/ã‚¿ã‚¤ãƒˆãƒ«ã€ç™ºç”Ÿæ™‚ã®çŠ¶æ³ã€å ±å‘Šè€…åã¯å¿…é ˆã§ã™ã€‚")
                return
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            filenames_json = json.dumps([f.name for f in file_attachments])
            urls_list = []
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†
            with st.spinner("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Cloud Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                for uploaded_file in file_attachments:
                    original_filename, public_url = upload_file_to_gcs(storage_client, uploaded_file, TROUBLE_FOLDER_NAME)
                    if public_url:
                        urls_list.append(public_url)

            urls_json = json.dumps(urls_list)
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—, æ©Ÿå™¨/å ´æ‰€, ç™ºç”Ÿæ—¥, ãƒˆãƒ©ãƒ–ãƒ«ç™ºç”Ÿæ™‚, åŸå› /ç©¶æ˜, å¯¾ç­–/å¾©æ—§, å†ç™ºé˜²æ­¢ç­–, å ±å‘Šè€…, ãƒ•ã‚¡ã‚¤ãƒ«å, ãƒ•ã‚¡ã‚¤ãƒ«URL, ä»¶å/ã‚¿ã‚¤ãƒˆãƒ«
            row_data = [
                timestamp, device, report_date.isoformat(), t_occur,
                t_cause, t_solution, t_prevention,
                reporter_name, filenames_json, urls_json, report_title # JSONæ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜
            ]
            
            # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿
            try:
                gc.open(SPREADSHEET_NAME).worksheet(SHEET_TROUBLE_DATA).append_row(row_data)
                st.success("ãƒˆãƒ©ãƒ–ãƒ«å ±å‘Šã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã—ã¾ã—ãŸã€‚")
                st.cache_data.clear(); st.rerun()
            except Exception as e:
                st.error(f"ãƒ‡ãƒ¼ã‚¿ã®æ›¸ãè¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚·ãƒ¼ãƒˆå '{SHEET_TROUBLE_DATA}' ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                st.exception(e)
                
    # --- ä¸€è¦§è¡¨ç¤º ---
    page_data_list(SHEET_TROUBLE_DATA, "ãƒˆãƒ©ãƒ–ãƒ«å ±å‘Š", TROUBLE_COL_TIMESTAMP, col_filter=TROUBLE_COL_DEVICE, detail_cols=[TROUBLE_COL_TITLE, TROUBLE_COL_OCCUR_DATE, TROUBLE_COL_CAUSE, TROUBLE_COL_SOLUTION, TROUBLE_COL_REPORTER, TROUBLE_COL_FILE_URL])


def page_contact():
    # ... (å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’ç¶­æŒ) ...
    st.header("âœ‰ï¸ é€£çµ¡ãƒ»å•ã„åˆã‚ã›")
    st.markdown("ã‚¢ãƒ—ãƒªç®¡ç†è€…ã¸ã®é€£çµ¡ã‚„ãƒã‚°å ±å‘Šã‚’è¡Œã„ã¾ã™ã€‚")
    
    # ... (å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶­æŒ) ...
    
    contact = st.text_input("é€£çµ¡å…ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹", key='contact_email')
    contact_type = st.selectbox("ãŠå•ã„åˆã‚ã›ã®ç¨®é¡", ["ãƒã‚°å ±å‘Š", "æ©Ÿèƒ½è¦æœ›", "ãã®ä»–"], key='contact_type')
    detail = st.text_area("è©³ç´°å†…å®¹", height=150, key='contact_detail')
    
    if st.button("é€ä¿¡", key='send_contact_button'):
        if not contact_type or not detail:
            st.error("ãŠå•ã„åˆã‚ã›ã®ç¨®é¡ã¨è©³ç´°å†…å®¹ã¯å¿…é ˆã§ã™ã€‚")
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        row_data = [
            timestamp, contact_type, detail, contact
        ]
        
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿
        try:
            gc.open(SPREADSHEET_NAME).worksheet(SHEET_CONTACT_DATA).append_row(row_data)
            st.success("ãŠå•ã„åˆã‚ã›ã‚’é€ä¿¡ã—ã¾ã—ãŸã€‚")
            st.cache_data.clear(); st.rerun()
        except Exception as e:
            st.error(f"ãƒ‡ãƒ¼ã‚¿ã®æ›¸ãè¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚·ãƒ¼ãƒˆå '{SHEET_CONTACT_DATA}' ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            st.exception(e)


# --------------------------------------------------------------------------
# --- Main App Execution ---
# --------------------------------------------------------------------------
def main():
    st.sidebar.title("å±±æ ¹ç ” ãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆ")
    
    # ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’è¨˜éŒ²ãƒ»ä¸€è¦§ã§çµ±åˆ
    menu_selection = st.sidebar.radio("æ©Ÿèƒ½é¸æŠ", [
        "ã‚¨ãƒ”ãƒãƒ¼ãƒˆ", "ãƒ¡ãƒ³ãƒ†ãƒãƒ¼ãƒˆ", "è­°äº‹éŒ²", "çŸ¥æµè¢‹ãƒ»è³ªå•ç®±", "è£…ç½®å¼•ãç¶™ããƒ¡ãƒ¢", "ãƒˆãƒ©ãƒ–ãƒ«å ±å‘Š", "é€£çµ¡ãƒ»å•ã„åˆã‚ã›",
        "âš¡ IVãƒ‡ãƒ¼ã‚¿è§£æ", "ğŸ”¬ PLãƒ‡ãƒ¼ã‚¿è§£æ", "ğŸ—“ï¸ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ»è£…ç½®äºˆç´„"
    ])
    
    # ãƒšãƒ¼ã‚¸ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
    if menu_selection == "ã‚¨ãƒ”ãƒãƒ¼ãƒˆ": page_epi_note()
    elif menu_selection == "ãƒ¡ãƒ³ãƒ†ãƒãƒ¼ãƒˆ": page_mainte_note()
    elif menu_selection == "è­°äº‹éŒ²": page_meeting_note()
    elif menu_selection == "çŸ¥æµè¢‹ãƒ»è³ªå•ç®±": page_qa_box()
    elif menu_selection == "è£…ç½®å¼•ãç¶™ããƒ¡ãƒ¢": page_handoff_notes()
    elif menu_selection == "ãƒˆãƒ©ãƒ–ãƒ«å ±å‘Š": page_trouble_report()
    elif menu_selection == "é€£çµ¡ãƒ»å•ã„åˆã‚ã›": page_contact()
    elif menu_selection == "âš¡ IVãƒ‡ãƒ¼ã‚¿è§£æ": page_iv_analysis()
    elif menu_selection == "ğŸ”¬ PLãƒ‡ãƒ¼ã‚¿è§£æ": page_pl_analysis()
    elif menu_selection == "ğŸ—“ï¸ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ»è£…ç½®äºˆç´„": page_calendar()

if __name__ == "__main__":
    main()

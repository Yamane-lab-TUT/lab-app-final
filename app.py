# --------------------------------------------------------------------------
# Yamane Lab Convenience Tool - Streamlit Application (app.py)
# ä¿®æ­£ç‰ˆ v20.7.0
#  - IVçµåˆãƒ‡ãƒ¼ã‚¿è£œé–“å¯¾å¿œæ¸ˆã¿
#  - PLè§£æãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¿®æ­£
#  - ç”»åƒã‚¤ãƒ³ãƒ©ã‚¤ãƒ³è¡¨ç¤ºï¼ˆè‡ªå‹•ãƒªã‚µã‚¤ã‚ºå¯¾å¿œï¼‰
#  - ãƒ¡ãƒ‹ãƒ¥ãƒ¼é †åºå¤‰æ›´ï¼ˆIV/PLã‚’ãƒ¡ãƒ³ãƒ†ãƒãƒ¼ãƒˆä¸‹ã«é…ç½®ï¼‰
# --------------------------------------------------------------------------

import streamlit as st
import gspread
import pandas as pd
import os
import io
import re
import json
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime, date, timedelta
from urllib.parse import quote as url_quote
from io import BytesIO
import calendar
import matplotlib.font_manager as fm

# GCSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from google.cloud import storage
except ImportError:
    st.error("âŒ `google-cloud-storage` ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    pass

# --- Matplotlib æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š ---
try:
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['font.sans-serif'] = ['Meiryo', 'IPAexGothic', 'Noto Sans CJK JP']
    plt.rcParams['axes.unicode_minus'] = False
except Exception:
    pass

st.set_page_config(page_title="å±±æ ¹ç ” ä¾¿åˆ©å±‹ã•ã‚“", layout="wide")

CLOUD_STORAGE_BUCKET_NAME = "yamane-lab-app-files"
SPREADSHEET_NAME = 'ã‚¨ãƒ”ãƒãƒ¼ãƒˆ'

# --- Google èªè¨¼ ---
class DummyGSClient:
    def open(self, name): return self
    def worksheet(self, name): return self
    def get_all_values(self): return []
    def append_row(self, values): pass

class DummyStorageClient:
    def bucket(self, name): return self
    def blob(self, name): return self
    def upload_from_file(self, f, content_type): pass

gc = DummyGSClient()
storage_client = DummyStorageClient()

@st.cache_resource(ttl=3600)
def initialize_google_services():
    if "gcs_credentials" not in st.secrets:
        st.warning("âš ï¸ Secretsã«`gcs_credentials`ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return DummyGSClient(), DummyStorageClient()
    try:
        info = json.loads(st.secrets["gcs_credentials"])
        gc_real = gspread.service_account_from_dict(info)
        storage_real = storage.Client.from_service_account_info(info)
        st.sidebar.success("âœ… Googleèªè¨¼æˆåŠŸ")
        return gc_real, storage_real
    except Exception as e:
        st.error(f"èªè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return DummyGSClient(), DummyStorageClient()

gc, storage_client = initialize_google_services()

# --------------------------------------------------------------------------
# ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•°
# --------------------------------------------------------------------------
@st.cache_data(ttl=600)
def get_sheet_as_df(spreadsheet_name, sheet_name):
    try:
        ws = gc.open(spreadsheet_name).worksheet(sheet_name)
        data = ws.get_all_values()
        if not data or len(data) <= 1:
            return pd.DataFrame(columns=data[0] if data else [])
        return pd.DataFrame(data[1:], columns=data[0])
    except Exception:
        return pd.DataFrame()

# --------------------------------------------------------------------------
# --- IV/PLå…±é€šãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ---
# --------------------------------------------------------------------------
def _load_two_column_data_core(uploaded_bytes, column_names):
    try:
        text = uploaded_bytes.decode('utf-8', errors='ignore').splitlines()
        data_lines = [l for l in text if l.strip() and not l.startswith(('#', '!', '/'))]
        if not data_lines: return None
        df = pd.read_csv(io.StringIO("\n".join(data_lines)),
                         sep=r'\s+|,|\t', engine='python', header=None)
        df = df.iloc[:, :2]
        df.columns = column_names
        df[column_names[0]] = pd.to_numeric(df[column_names[0]], errors='coerce')
        df[column_names[1]] = pd.to_numeric(df[column_names[1]], errors='coerce')
        df = df.dropna().sort_values(column_names[0])
        return df
    except Exception:
        return None

@st.cache_data(show_spinner="IVãƒ‡ãƒ¼ã‚¿è§£æä¸­...")
def load_data_file(uploaded_bytes, filename):
    return _load_two_column_data_core(uploaded_bytes, ['Axis_X', filename])

@st.cache_data(show_spinner="PLãƒ‡ãƒ¼ã‚¿è§£æä¸­...")
def load_pl_data(uploaded_file):
    try:
        file_bytes = uploaded_file.read()
        uploaded_file.seek(0)
        df = _load_two_column_data_core(file_bytes, ['pixel', 'intensity'])
        if df is not None and not df.empty:
            return df[['pixel', 'intensity']]
    except Exception as e:
        st.error(f"PLãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    return None

# --------------------------------------------------------------------------
# --- IVãƒ‡ãƒ¼ã‚¿çµåˆï¼ˆæ”¹è‰¯ç‰ˆãƒ»è£œé–“å¯¾å¿œï¼‰ ---
# --------------------------------------------------------------------------
@st.cache_data(show_spinner="IVãƒ‡ãƒ¼ã‚¿çµåˆä¸­...")
def combine_dataframes(dataframes, filenames, num_points=500):
    if not dataframes:
        return None
    all_x = np.concatenate([df['Axis_X'].values for df in dataframes])
    x_common = np.linspace(all_x.min(), all_x.max(), num_points)
    combined_df = pd.DataFrame({'X_Axis': x_common})
    for df, name in zip(dataframes, filenames):
        df_sorted = df.sort_values('Axis_X')
        y_interp = np.interp(x_common, df_sorted['Axis_X'], df_sorted.iloc[:, 1])
        combined_df[name] = y_interp
    return combined_df

# --------------------------------------------------------------------------
# --- æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«è¡¨ç¤ºï¼ˆè‡ªå‹•ç”»åƒãƒªã‚µã‚¤ã‚ºä»˜ãï¼‰ ---
# --------------------------------------------------------------------------
def display_attached_files(row, col_url, col_filename=None):
    if col_url not in row or not row[col_url]:
        st.info("æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    try:
        urls = json.loads(row[col_url])
        filenames = []
        if col_filename and row.get(col_filename):
            filenames = json.loads(row[col_filename])
        if not filenames:
            filenames = ['ãƒ•ã‚¡ã‚¤ãƒ«'] * len(urls)
        for filename, url in zip(filenames, urls):
            if not url:
                continue
            is_image = url.lower().endswith(('.png', '.jpg', '.jpeg'))
            if is_image:
                st.image(url, caption=filename, use_container_width=True)
            else:
                st.markdown(f"ğŸ”— [{filename}]({url})")
    except Exception:
        st.markdown("âš ï¸ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

# --------------------------------------------------------------------------
# --- å„ãƒšãƒ¼ã‚¸ï¼ˆIV/PLè§£æã®ã¿å†æ²ï¼‰ ---
# --------------------------------------------------------------------------
def page_iv_analysis():
    st.header("âš¡ IVãƒ‡ãƒ¼ã‚¿è§£æ")
    uploaded_files = st.file_uploader("IVãƒ‡ãƒ¼ã‚¿ (.txt)", type=['txt'], accept_multiple_files=True)
    if not uploaded_files:
        st.info("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return

    valid_dfs, names = [], []
    for f in uploaded_files:
        df = load_data_file(f.getvalue(), f.name)
        if df is not None and not df.empty:
            valid_dfs.append(df)
            names.append(f.name)

    if not valid_dfs:
        st.warning("æœ‰åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    combined_df = combine_dataframes(valid_dfs, names)
    st.success(f"{len(valid_dfs)}ä»¶ã‚’çµåˆã—ã¾ã—ãŸã€‚")

    fig, ax = plt.subplots(figsize=(10,6))
    for name in names:
        ax.plot(combined_df['X_Axis'], combined_df[name], label=name)
    ax.set_xlabel("é›»åœ§ (V)")
    ax.set_ylabel("é›»æµ (A)")
    ax.legend()
    ax.grid(True)
    st.pyplot(fig)

    st.dataframe(combined_df.head(), use_container_width=True)
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        combined_df.to_excel(writer, index=False, sheet_name="IV_combined")
    st.download_button("ğŸ’¾ Excelãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", output.getvalue(),
                       f"iv_combined_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx")

def page_pl_analysis():
    st.header("ğŸ”¬ PLãƒ‡ãƒ¼ã‚¿è§£æ")
    st.write("æ ¡æ­£â†’ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æã®2ã‚¹ãƒ†ãƒƒãƒ—ã§è¡Œã„ã¾ã™ã€‚")

    # --- Step 1 æ ¡æ­£ ---
    cal1_wl = st.number_input("åŸºæº–æ³¢é•·1 (nm)", value=1500)
    cal2_wl = st.number_input("åŸºæº–æ³¢é•·2 (nm)", value=1570)
    cal1_file = st.file_uploader(f"{cal1_wl} nm æ ¡æ­£ãƒ•ã‚¡ã‚¤ãƒ«", type=['txt'], key="cal1")
    cal2_file = st.file_uploader(f"{cal2_wl} nm æ ¡æ­£ãƒ•ã‚¡ã‚¤ãƒ«", type=['txt'], key="cal2")

    if st.button("æ ¡æ­£å®Ÿè¡Œ"):
        if cal1_file and cal2_file:
            df1, df2 = load_pl_data(cal1_file), load_pl_data(cal2_file)
            if df1 is not None and df2 is not None:
                p1 = df1['pixel'].iloc[df1['intensity'].idxmax()]
                p2 = df2['pixel'].iloc[df2['intensity'].idxmax()]
                slope = (cal2_wl - cal1_wl) / (p1 - p2)
                st.session_state['pl_slope'] = slope
                st.session_state['pl_calibrated'] = True
                st.success(f"æ ¡æ­£å®Œäº†: {slope:.4f} nm/pixel")
            else:
                st.error("æ ¡æ­£ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        else:
            st.warning("2ã¤ã®æ ¡æ­£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")

    st.write("---")
    st.subheader("ã‚¹ãƒ†ãƒƒãƒ—2ï¼šæ¸¬å®šãƒ‡ãƒ¼ã‚¿è§£æ")

    if not st.session_state.get('pl_calibrated', False):
        st.info("ğŸ’¡ ã¾ãšæ ¡æ­£ã‚’å®Œäº†ã—ã¦ãã ã•ã„ã€‚")
        return

    center_wl = st.number_input("æ¸¬å®šæ™‚ã®ä¸­å¿ƒæ³¢é•· (nm)", value=1700)
    uploaded_files = st.file_uploader("PLæ¸¬å®šãƒ‡ãƒ¼ã‚¿ (.txt)", type=['txt'], accept_multiple_files=True)
    if not uploaded_files:
        return

    slope = st.session_state['pl_slope']
    center_pixel = 256.5

    fig, ax = plt.subplots(figsize=(10,6))
    merged = None
    for f in uploaded_files:
        df = load_pl_data(f)
        if df is None: continue
        df['wavelength_nm'] = (df['pixel'] - center_pixel) * slope + center_wl
        ax.plot(df['wavelength_nm'], df['intensity'], label=f.name)
        df = df[['wavelength_nm', 'intensity']].rename(columns={'intensity': f.name})
        merged = df if merged is None else pd.merge(merged, df, on='wavelength_nm', how='outer')

    ax.set_xlabel("æ³¢é•· (nm)")
    ax.set_ylabel("PLå¼·åº¦ (a.u.)")
    ax.legend()
    ax.grid(True)
    st.pyplot(fig)
    if merged is not None:
        merged = merged.sort_values('wavelength_nm')
        output = BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            merged.to_excel(writer, index=False, sheet_name="PL_combined")
        st.download_button("ğŸ“Š Excelãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", output.getvalue(),
                           f"pl_combined_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx")

# --------------------------------------------------------------------------
# --- ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚° ---
# --------------------------------------------------------------------------
def main():
    st.sidebar.title("å±±æ ¹ç ” ãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆ")
    menu = st.sidebar.radio("æ©Ÿèƒ½é¸æŠ", [
        "ã‚¨ãƒ”ãƒãƒ¼ãƒˆ",
        "ãƒ¡ãƒ³ãƒ†ãƒãƒ¼ãƒˆ",
        "âš¡ IVãƒ‡ãƒ¼ã‚¿è§£æ",
        "ğŸ”¬ PLãƒ‡ãƒ¼ã‚¿è§£æ",
        "è­°äº‹éŒ²",
        "çŸ¥æµè¢‹ãƒ»è³ªå•ç®±",
        "è£…ç½®å¼•ãç¶™ããƒ¡ãƒ¢",
        "ãƒˆãƒ©ãƒ–ãƒ«å ±å‘Š",
        "é€£çµ¡ãƒ»å•ã„åˆã‚ã›",
        "ğŸ—“ï¸ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ»è£…ç½®äºˆç´„"
    ])

    if menu == "âš¡ IVãƒ‡ãƒ¼ã‚¿è§£æ":
        page_iv_analysis()
    elif menu == "ğŸ”¬ PLãƒ‡ãƒ¼ã‚¿è§£æ":
        page_pl_analysis()
    else:
        st.info("ã“ã®æ©Ÿèƒ½ã¯ç¾åœ¨çœç•¥ã—ã¦ã„ã¾ã™ï¼ˆæ—¢å­˜ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‹ã‚‰æµç”¨å¯èƒ½ï¼‰ã€‚")

if __name__ == "__main__":
    main()

# -*- coding: utf-8 -*-
"""
Yamane Lab Convenience Tool - Complete Fixed Version + High-End Graph Plotter
æ©Ÿèƒ½: ã‚¨ãƒ”ãƒãƒ¼ãƒˆ/ãƒ¡ãƒ³ãƒ†ãƒãƒ¼ãƒˆ/ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼/è§£æ(IV, PL)/è­°äº‹éŒ²/çŸ¥æµè¢‹/å¼•ãç¶™ã/ãƒˆãƒ©ãƒ–ãƒ«/å•ã„åˆã‚ã›/ã€Newã€‘ã‚°ãƒ©ãƒ•æç”»
"""

import streamlit as st
import gspread
import pandas as pd
import os
import io
import re
import json
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import numpy as np
from datetime import datetime, date, timedelta
from urllib.parse import quote as url_quote, unquote as url_unquote
from io import BytesIO
import calendar
import matplotlib.font_manager as fm
from functools import reduce

# Google Services
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# Optional GCS
try:
    from google.cloud import storage
except Exception:
    storage = None

# --- Matplotlib æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š ---
try:
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['font.sans-serif'] = [
        'Hiragino Maru Gothic Pro', 'Yu Gothic', 'Meiryo',
        'TakaoGothic', 'IPAexGothic', 'Noto Sans CJK JP'
    ]
    plt.rcParams['axes.unicode_minus'] = False
except Exception:
    pass

# --- Streamlit ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="å±±æ ¹ç ” ä¾¿åˆ©å±‹ã•ã‚“", layout="wide")

# ---------------------------
# --- Constants ---
# ---------------------------
CLOUD_STORAGE_BUCKET_NAME = "yamane-lab-app-files"
SPREADSHEET_NAME = "ã‚¨ãƒ”ãƒãƒ¼ãƒˆ"

# ã‚·ãƒ¼ãƒˆå®šç¾© (çœç•¥ - ãã®ã¾ã¾ç¶­æŒ)
SHEET_EPI_DATA = 'ã‚¨ãƒ”ãƒãƒ¼ãƒˆ_ãƒ‡ãƒ¼ã‚¿'
EPI_COL_TIMESTAMP = 'ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—'
EPI_COL_CATEGORY = 'ã‚«ãƒ†ã‚´ãƒª'
EPI_COL_MEMO = 'ãƒ¡ãƒ¢'
EPI_COL_FILENAME = 'ãƒ•ã‚¡ã‚¤ãƒ«å'
EPI_COL_FILE_URL = 'å†™çœŸURL'

SHEET_MAINTE_DATA = 'ãƒ¡ãƒ³ãƒ†ãƒãƒ¼ãƒˆ_ãƒ‡ãƒ¼ã‚¿'
MAINT_COL_TIMESTAMP = 'ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—'
MAINT_COL_MEMO = 'ãƒ¡ãƒ¢'
MAINT_COL_FILENAME = 'ãƒ•ã‚¡ã‚¤ãƒ«å'
MAINT_COL_FILE_URL = 'å†™çœŸURL'

SHEET_MEETING_DATA = 'è­°äº‹éŒ²_ãƒ‡ãƒ¼ã‚¿'
MEETING_COL_TIMESTAMP = 'ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—'
MEETING_COL_TITLE = 'ä¼šè­°ã‚¿ã‚¤ãƒˆãƒ«'
MEETING_COL_AUDIO_URL = 'éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«URL'
MEETING_COL_CONTENT = 'è­°äº‹éŒ²å†…å®¹'

SHEET_HANDOVER_DATA = 'å¼•ãç¶™ã_ãƒ‡ãƒ¼ã‚¿'
HANDOVER_COL_TIMESTAMP = 'ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—'
HANDOVER_COL_TYPE = 'ç¨®é¡'
HANDOVER_COL_TITLE = 'ã‚¿ã‚¤ãƒˆãƒ«'
HANDOVER_COL_MEMO = 'ãƒ¡ãƒ¢'

SHEET_QA_DATA = 'çŸ¥æµè¢‹_ãƒ‡ãƒ¼ã‚¿'
QA_COL_TIMESTAMP = 'ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—'
QA_COL_TITLE = 'è³ªå•ã‚¿ã‚¤ãƒˆãƒ«'
QA_COL_CONTENT = 'è³ªå•å†…å®¹'
QA_COL_CONTACT = 'é€£çµ¡å…ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹'
QA_COL_FILENAME = 'æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«å'
QA_COL_FILE_URL = 'æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«URL'
QA_COL_STATUS = 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹'

SHEET_CONTACT_DATA = 'ãŠå•ã„åˆã‚ã›_ãƒ‡ãƒ¼ã‚¿'
CONTACT_COL_TIMESTAMP = 'ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—'
CONTACT_COL_TYPE = 'ãŠå•ã„åˆã‚ã›ã®ç¨®é¡'
CONTACT_COL_DETAIL = 'è©³ç´°å†…å®¹'
CONTACT_COL_CONTACT = 'é€£çµ¡å…ˆ'

SHEET_TROUBLE_DATA = 'ãƒˆãƒ©ãƒ–ãƒ«å ±å‘Š_ãƒ‡ãƒ¼ã‚¿'
TROUBLE_COL_TIMESTAMP = 'ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—'
TROUBLE_COL_DEVICE = 'æ©Ÿå™¨/å ´æ‰€'
TROUBLE_COL_TITLE = 'ä»¶å/ã‚¿ã‚¤ãƒˆãƒ«'
TROUBLE_COL_CAUSE = 'åŸå› /ç©¶æ˜'
TROUBLE_COL_SOLUTION = 'å¯¾ç­–/å¾©æ—§'
TROUBLE_COL_REPORTER = 'å ±å‘Šè€…'

# Calendar Config
CALENDAR_ID = "yamane.lab.6747@gmail.com"
SCOPES = ['https://www.googleapis.com/auth/calendar']

# ---------------------------
# --- Service Classes ---
# ---------------------------
class DummyGSClient:
    def open(self, name): return self
    def worksheet(self, name): return self
    def get_all_records(self): return []
    def get_all_values(self): return []
    def append_row(self, values): pass

class DummyStorageClient:
    def bucket(self, name): return self
    def blob(self, name): return self
    def list_blobs(self, **kwargs): return []
    def generate_signed_url(self, **kwargs): return None

# ---------------------------
# --- Initialization ---
# ---------------------------
@st.cache_resource(ttl=3600)
def initialize_google_services():
    global storage
    gc_client = DummyGSClient()
    storage_client_obj = DummyStorageClient()
    calendar_service = None

    if "gcs_credentials" not in st.secrets:
        # st.sidebar.warning("âš ï¸ Secretsæœªè¨­å®š (ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ¼ãƒ‰)")
        return gc_client, storage_client_obj, calendar_service

    try:
        raw = st.secrets["gcs_credentials"]
        cleaned = raw.strip().replace('\t', '').replace('\r', '').replace('\n', '')
        info = json.loads(cleaned)
        
        gc_client = gspread.service_account_from_dict(info)
        if storage:
            storage_client_obj = storage.Client.from_service_account_info(info)
        
        creds = service_account.Credentials.from_service_account_info(info, scopes=SCOPES)
        calendar_service = build('calendar', 'v3', credentials=creds)
        
        # st.sidebar.success("âœ… Googleã‚µãƒ¼ãƒ“ã‚¹èªè¨¼ æˆåŠŸ")
        return gc_client, storage_client_obj, calendar_service

    except Exception:
        # st.sidebar.error(f"Googleã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return gc_client, storage_client_obj, calendar_service

gc, storage_client, calendar_service = initialize_google_services()

# ---------------------------
# --- Utils ---
# ---------------------------
def upload_file_to_gcs(storage_client_obj, file_obj):
    if isinstance(storage_client_obj, DummyStorageClient) or storage is None:
        return None, None
    try:
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        original_filename = file_obj.name
        safe_filename = re.sub(r'[^a-zA-Z0-9_.]', '_', original_filename)
        gcs_filename = f"{timestamp}_{safe_filename}"
        
        bucket = storage_client_obj.bucket(CLOUD_STORAGE_BUCKET_NAME)
        blob = bucket.blob(gcs_filename)
        blob.upload_from_string(
            file_obj.getvalue(), 
            content_type=file_obj.type if hasattr(file_obj, 'type') else 'application/octet-stream'
        )
        public_url = f"https://storage.googleapis.com/{CLOUD_STORAGE_BUCKET_NAME}/{url_quote(gcs_filename)}"
        return original_filename, public_url
    except Exception:
        return None, None

def generate_signed_url(blob_name_quoted, expiration_minutes=15):
    if isinstance(storage_client, DummyStorageClient): return None
    try:
        bucket = storage_client.bucket(CLOUD_STORAGE_BUCKET_NAME)
        blob = bucket.blob(blob_name_quoted)
        return blob.generate_signed_url(version="v4", expiration=timedelta(minutes=expiration_minutes), method="GET")
    except Exception:
        return None

@st.cache_data(ttl=600)
def get_sheet_as_df(spreadsheet_name, sheet_name):
    try:
        if isinstance(gc, DummyGSClient): return pd.DataFrame()
        ws = gc.open(spreadsheet_name).worksheet(sheet_name)
        data = ws.get_all_values()
        if not data or len(data) <= 1: return pd.DataFrame()
        return pd.DataFrame(data[1:], columns=data[0])
    except Exception:
        return pd.DataFrame()

def display_attached_files(row, col_url, col_filename):
    raw_urls = row.get(col_url, '')
    raw_names = row.get(col_filename, '')
    urls = []
    names = []
    
    try:
        urls = json.loads(raw_urls) if raw_urls else []
        if not isinstance(urls, list): urls = [raw_urls] if isinstance(raw_urls, str) else []
    except:
        if raw_urls and raw_urls.startswith('http'): urls = [raw_urls]
        
    try:
        names = json.loads(raw_names) if raw_names else []
        if not isinstance(names, list): names = [names] if isinstance(names, str) else []
    except:
        pass

    while len(names) < len(urls): names.append(f"File {len(names)+1}")
    
    if urls:
        st.markdown("##### ğŸ“ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«")
        
        for u, n in zip(urls, names):
            is_image = n.lower().endswith(('.png', '.jpg', '.jpeg', '.gif'))
            
            blob_name_quoted = None
            if u.startswith(f"https://storage.googleapis.com/{CLOUD_STORAGE_BUCKET_NAME}/"):
                blob_name_quoted = u.split(f"https://storage.googleapis.com/{CLOUD_STORAGE_BUCKET_NAME}/")[1]

            if is_image and blob_name_quoted:
                signed_url = generate_signed_url(blob_name_quoted) 
                
                if signed_url:
                    st.image(signed_url, caption=f"ç”»åƒ: {n}", width=400)
                else:
                    st.markdown(f"- **ç”»åƒ ({n})** : GCSã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—ã€ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«æœŸé™åˆ‡ã‚Œã®ãŸã‚ [ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯]({u})")
            else:
                st.markdown(f"- [{n}]({u})")

# --- Excel Export Helpers ---
def to_excel(df):
    output = BytesIO()
    df = df.apply(pd.to_numeric, errors='coerce').astype(float)
    if 'Axis_X' in df.columns: df.rename(columns={'Axis_X': 'Voltage_V'}, inplace=True)
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False, sheet_name='Combined Data') 
    processed_data = output.getvalue()
    return processed_data

def to_excel_multi_sheet(data_dict):
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        for sheet_name, df in data_dict.items():
            export_df = df.apply(pd.to_numeric, errors='coerce').astype(float)
            if 'Axis_X' in export_df.columns:
                 export_df.rename(columns={'Axis_X': 'Voltage_V'}, inplace=True)
            export_df.to_excel(writer, index=False, sheet_name=sheet_name)
    processed_data = output.getvalue()
    return processed_data

# ---------------------------
# --- Data Loaders ---
# ---------------------------
@st.cache_data
def load_data_file(uploaded_bytes, filename):
    try:
        text = uploaded_bytes.decode('utf-8', errors='ignore').splitlines()
        data_lines = [line.strip() for line in text if line.strip() and not line.strip().startswith(('#','!','/'))]
        if data_lines and not data_lines[0][0].isdigit():
            data_lines = data_lines[1:]
            
        df = pd.read_csv(io.StringIO("\n".join(data_lines)), sep=r'\s+|,|\t', engine='python', header=None)
        if df.shape[1] < 2: return None
        df = df.iloc[:, :2]
        df.columns = ['Axis_X', filename]
        df = df.apply(pd.to_numeric, errors='coerce').dropna()
        return df
    except:
        return None

@st.cache_data
def load_pl_data(uploaded_file):
    try:
        content = uploaded_file.getvalue().decode('utf-8', errors='ignore').splitlines()
        
        data_lines = []
        for line in content:
            line = line.strip()
            if not line: continue
            if line.startswith(('#', '!', '/')): continue
            data_lines.append(line)
            
        if not data_lines: return None

        df = pd.read_csv(io.StringIO("\n".join(data_lines)), 
                         sep=r'[\t, ]+', 
                         engine='python', 
                         header=None)

        if df.shape[1] < 2: 
            df = df.dropna(axis=1, how='all')
            if df.shape[1] < 2:
                return None
        
        df = df.iloc[:, :2]
        df.columns = ['pixel', 'intensity']
        df = df.apply(pd.to_numeric, errors='coerce').dropna()
        if df.empty: return None
        return df
    except Exception:
        return None
        
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import json
import io
import uuid
from scipy import stats
from datetime import datetime
from io import BytesIO

# ==========================================
# é–¢æ•°å®šç¾©: page_graph_plotting (v20: å…¨åˆ—å±•é–‹å–ã‚Šè¾¼ã¿ãƒ»è¤‡è£½æ©Ÿèƒ½è¿½åŠ ç‰ˆ)
# ==========================================
def page_graph_plotting():
    st.header("ğŸ“ˆ çµ±åˆå‹ã‚°ãƒ©ãƒ•è§£æãƒ„ãƒ¼ãƒ«")
    st.markdown("""
    **v20 æ›´æ–°**: 
    - **å…¨åˆ—æ´»ç”¨**: 3åˆ—ä»¥ä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æ™‚ã€å…¨ã¦ã®åˆ—ã‚’åˆ¥ã€…ã®ç³»åˆ—ã¨ã—ã¦ä¸€æ‹¬è¿½åŠ ã§ãã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚
    - **è¤‡è£½æ©Ÿèƒ½**: ç™»éŒ²æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ç³»åˆ—ã‚’ã‚³ãƒ”ãƒ¼ã™ã‚‹ãƒœã‚¿ãƒ³ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼ˆåŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ã§X/Yè»¸ã‚’å¤‰ãˆã¦è¡¨ç¤ºã—ãŸã„å ´åˆã«ä¾¿åˆ©ã§ã™ï¼‰ã€‚
    - **ã‚µã‚¤ã‚ºæŒ‡å®š**: cmå˜ä½ãƒ»å®Ÿå¯¸ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»ãƒ•ã‚¡ã‚¤ãƒ«åæŒ‡å®šä¿å­˜ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚
    """, unsafe_allow_html=True)

    # --- CSS ---
    st.markdown("""
        <style>
        div[data-testid="stHorizontalBlock"] > div[data-testid="stColumn"]:nth-of-type(2) {
            position: sticky; top: 4rem; align-self: start; z-index: 999;
        }
        div[data-testid="stMarkdownContainer"] p { margin-bottom: 0px; }
        </style>
    """, unsafe_allow_html=True)

    # --- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè‰² ---
    DEFAULT_COLORS = [
        '#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', 
        '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'
    ]

    # --- ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ– ---
    if 'gp_data_list' not in st.session_state:
        st.session_state['gp_data_list'] = []
    
    if 'uploader_key_id' not in st.session_state:
        st.session_state['uploader_key_id'] = 0

    # --- IDãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ ---
    for d in st.session_state['gp_data_list']:
        if 'id' not in d: d['id'] = str(uuid.uuid4())

    # --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
    def move_data(idx, direction):
        lst = st.session_state['gp_data_list']
        if direction == "up" and idx > 0:
            lst[idx], lst[idx-1] = lst[idx-1], lst[idx]
        elif direction == "down" and idx < len(lst) - 1:
            lst[idx], lst[idx+1] = lst[idx+1], lst[idx]

    def duplicate_data(idx):
        lst = st.session_state['gp_data_list']
        original = lst[idx]
        # è¾æ›¸ã‚’æµ…ã„ã‚³ãƒ”ãƒ¼ï¼ˆDataFrameã¯å‚ç…§æ¸¡ã—ã§ãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
        new_item = original.copy()
        new_item['id'] = str(uuid.uuid4())
        new_item['legend_name'] = f"{original.get('legend_name', '')} (copy)"
        # ãƒªã‚¹ãƒˆã®ç›´ä¸‹ã«æŒ¿å…¥
        lst.insert(idx + 1, new_item)

    def get_next_color(index):
        return DEFAULT_COLORS[index % len(DEFAULT_COLORS)]

    def format_power(watts):
        if watts == 0: return "0 W"
        w_abs = abs(watts)
        if w_abs >= 1: return f"{watts:.3f} W"
        elif w_abs >= 1e-3: return f"{watts*1e3:.3f} mW"
        elif w_abs >= 1e-6: return f"{watts*1e6:.3f} ÂµW"
        elif w_abs >= 1e-9: return f"{watts*1e9:.3f} nW"
        else: return f"{watts*1e12:.3f} pW"

    # ==========================================
    # 0. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†
    # ==========================================
    with st.expander("ğŸ’¾ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¿å­˜ãƒ»å¾©å…ƒ", expanded=False):
        c_load, c_save = st.columns(2)
        with c_load:
            st.markdown("#### ğŸ“‚ å¾©å…ƒ")
            uploaded_project = st.file_uploader("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ« (.json)", type=["json"], key="project_loader_v20")
            if uploaded_project:
                if st.button("è¨­å®šã‚’èª­ã¿è¾¼ã‚€", key="btn_load_proj_v20"):
                    try:
                        project_data = json.load(uploaded_project)
                        restored_data_list = []
                        for item in project_data.get("datasets", []):
                            df_restored = pd.read_csv(io.StringIO(item["data_csv"]))
                            item['df'] = df_restored
                            cols = df_restored.columns.tolist()
                            
                            defaults = {
                                "mppt": False, "show_eq": False, "visible": True, 
                                "legend_name": item.get('name', ''),
                                "id": str(uuid.uuid4()),
                                "x_col": cols[0] if cols else None,
                                "y_col": cols[1] if len(cols)>1 else (cols[0] if cols else None),
                                "area": 1.0, "use_density": False,
                                "mppt_x": 10, "mppt_y": -30,
                                "fill_area": False
                            }
                            for k, v in defaults.items():
                                if k not in item: item[k] = v
                                
                            restored_data_list.append(item)
                        
                        st.session_state['gp_data_list'] = restored_data_list
                        saved_settings = project_data.get("settings", {})
                        for key, value in saved_settings.items():
                            st.session_state[key] = value
                        st.success("âœ… å¾©å…ƒå®Œäº†")
                        st.rerun()
                    except Exception as e: st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

        with c_save:
            st.markdown("#### ğŸ’¾ ä¿å­˜")
            default_proj_name = f"GraphProject_{datetime.now().strftime('%Y%m%d_%H%M')}"
            save_name_proj = st.text_input("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå (æ‹¡å¼µå­ä¸è¦)", value=default_proj_name, key="proj_save_name_v20")
            
            if st.button("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ", key="btn_save_proj_v20"):
                if not st.session_state['gp_data_list']:
                    st.warning("ãƒ‡ãƒ¼ã‚¿ãªã—")
                else:
                    datasets_serialized = []
                    for d in st.session_state['gp_data_list']:
                        csv_buffer = io.StringIO()
                        d['df'].to_csv(csv_buffer, index=False)
                        d_copy = d.copy()
                        d_copy['data_csv'] = csv_buffer.getvalue()
                        if 'df' in d_copy: del d_copy['df']
                        datasets_serialized.append(d_copy)
                    
                    settings_snapshot = {}
                    for key, val in st.session_state.items():
                        # ä¿å­˜å¯¾è±¡å¤–ã‚­ãƒ¼
                        if key.startswith(("project_", "gp_", "btn_", "paste_", "fw_", "fh_", "dpi_", "ff_", "bfs_", "sleg", "lfont", "ax_preset", "legend_", "scale_sel", "vis_", "leg_nm_", "xc_", "yc_", "ut_", "ur_", "clr_", "mrk_", "lw_", "ms_", "lst_", "mppt_", "fit_", "seq_", "area_", "dens_", "mx_", "my_", "fill_", "xy_swap_", "proj_save_name", "img_save_name")): continue
                        if isinstance(val, (int, float, str, bool, list, dict, type(None))):
                            settings_snapshot[key] = val

                    project_obj = {
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "datasets": datasets_serialized,
                        "settings": settings_snapshot
                    }
                    json_str = json.dumps(project_obj, indent=2, ensure_ascii=False)
                    
                    final_proj_fname = save_name_proj.strip()
                    if not final_proj_fname: final_proj_fname = default_proj_name
                    if not final_proj_fname.endswith(".json"): final_proj_fname += ".json"
                    
                    st.download_button("â¬‡ï¸ JSONã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", json_str, final_proj_fname, "application/json", key="dl_json_btn_v20")

    # ==========================================
    # 1. ãƒ‡ãƒ¼ã‚¿å…¥åŠ›
    # ==========================================
    st.subheader("1. ãƒ‡ãƒ¼ã‚¿ã®å…¥åŠ›")
    
    if st.session_state['gp_data_list']:
        st.info(f"ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿æ•°: {len(st.session_state['gp_data_list'])}")
        if st.button("ğŸ—‘ï¸ å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢", key="btn_clear_all_v20"):
            st.session_state['gp_data_list'] = []
            st.session_state['uploader_key_id'] += 1
            st.rerun()
    
    tab1, tab2 = st.tabs(["ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¿½åŠ ", "ğŸ“‹ ã‚¨ã‚¯ã‚»ãƒ«ã‹ã‚‰è²¼ã‚Šä»˜ã‘"])
    
    with tab1:
        st.markdown("**èª­ã¿è¾¼ã¿ã‚ªãƒ—ã‚·ãƒ§ãƒ³**")
        # --- å…¨åˆ—å±•é–‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ ---
        expand_cols = st.checkbox("åˆ—ã”ã¨ã«åˆ¥ã®ç³»åˆ—ã¨ã—ã¦è¿½åŠ ã™ã‚‹ï¼ˆä¾‹: Aåˆ—vsBåˆ—, Aåˆ—vsCåˆ—...ï¼‰", value=False, help="ãƒã‚§ãƒƒã‚¯ã‚’å…¥ã‚Œã‚‹ã¨ã€2åˆ—ç›®ä»¥é™ã®ã™ã¹ã¦ã®åˆ—ã‚’ã€1åˆ—ç›®ã‚’Xè»¸ã¨ã—ãŸå€‹åˆ¥ã®ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¸€æ‹¬ã§è¿½åŠ ã—ã¾ã™ã€‚", key="expand_cols_v20")
        
        current_uploader_key = f"gp_uploader_v20_{st.session_state['uploader_key_id']}"
        files = st.file_uploader("CSV/Excelãƒ•ã‚¡ã‚¤ãƒ«", accept_multiple_files=True, key=current_uploader_key)
        
        if files:
            new_data_added = False
            for f in files:
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆåå‰ã ã‘ã§åˆ¤å®šï¼‰- å±•é–‹ãƒ¢ãƒ¼ãƒ‰ã®ã¨ãã¯åå‰ãŒå¤‰ã‚ã‚‹ã®ã§ç·©ã‚ã‚‹
                if not expand_cols and any(d['name'] == f.name for d in st.session_state['gp_data_list']): continue
                
                df = None
                try:
                    if f.name.endswith(('.xlsx', '.xls')): df = pd.read_excel(f)
                    else: df = pd.read_csv(f)
                except: pass
                
                if df is not None:
                    # æ•°å€¤åˆ—ã®ã¿æŠ½å‡º
                    df = df.select_dtypes(include=[np.number])
                    df.columns = [str(c).strip() for c in df.columns]
                    cols = df.columns.tolist()
                    
                    if not cols: continue

                    # è¿½åŠ ãƒ­ã‚¸ãƒƒã‚¯
                    if expand_cols and len(cols) >= 2:
                        # å±•é–‹ãƒ¢ãƒ¼ãƒ‰: 1åˆ—ç›®ã‚’Xã¨ã—ã¦ã€2åˆ—ç›®ä»¥é™ã™ã¹ã¦ã‚’Yã¨ã—ã¦ç™»éŒ²
                        x_c = cols[0]
                        for y_c in cols[1:]:
                            auto_color = get_next_color(len(st.session_state['gp_data_list']))
                            st.session_state['gp_data_list'].append({
                                "id": str(uuid.uuid4()),
                                "name": f.name,
                                "df": df, # åŒã˜DFã‚’å‚ç…§
                                "legend_name": f"{f.name} ({y_c})", # å‡¡ä¾‹ã«åˆ—åã‚’å«ã‚ã‚‹
                                "mppt": False, "show_eq": False, "visible": True,
                                "color": auto_color, "marker": "None", "linestyle": "-",
                                "x_col": x_c,
                                "y_col": y_c,
                                "area": 1.0, "use_density": False,
                                "mppt_x": 10, "mppt_y": -30, "fill_area": False
                            })
                        new_data_added = True
                    else:
                        # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: 1ã¤ã®ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ç™»éŒ²ï¼ˆåˆæœŸã¯Col1 vs Col2ï¼‰
                        auto_color = get_next_color(len(st.session_state['gp_data_list']))
                        st.session_state['gp_data_list'].append({
                            "id": str(uuid.uuid4()),
                            "name": f.name, "df": df,
                            "legend_name": f.name,
                            "mppt": False, "show_eq": False, "visible": True,
                            "color": auto_color, "marker": "None", "linestyle": "-",
                            "x_col": cols[0] if cols else None,
                            "y_col": cols[1] if len(cols) > 1 else (cols[0] if cols else None),
                            "area": 1.0, "use_density": False,
                            "mppt_x": 10, "mppt_y": -30, "fill_area": False
                        })
                        new_data_added = True

            if new_data_added: st.rerun()

    with tab2:
        st.caption("Excelã‹ã‚‰ã‚³ãƒ”ãƒš (ã‚¿ãƒ–åŒºåˆ‡ã‚Š) ã—ã¦ Ctrl+Enter")
        paste_text = st.text_area("ãƒ‡ãƒ¼ã‚¿è²¼ã‚Šä»˜ã‘ã‚¨ãƒªã‚¢", height=100, key="paste_area_v20")
        paste_name = st.text_input("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå", value=f"Data_{len(st.session_state['gp_data_list'])+1}", key="paste_name_v20")
        
        if st.button("è²¼ã‚Šä»˜ã‘è¿½åŠ ", key="btn_paste_add_v20"):
            if paste_text:
                try:
                    df_paste = pd.read_csv(io.StringIO(paste_text), sep='\t')
                    if df_paste is not None and not df_paste.empty:
                        df_paste = df_paste.select_dtypes(include=[np.number])
                        cols = df_paste.columns.tolist()
                        auto_color = get_next_color(len(st.session_state['gp_data_list']))
                        
                        st.session_state['gp_data_list'].append({
                            "id": str(uuid.uuid4()),
                            "name": paste_name, "df": df_paste,
                            "legend_name": paste_name,
                            "mppt": False, "show_eq": False,
                            "visible": True,
                            "color": auto_color, "marker": "None",
                            "linestyle": "-",
                            "x_col": cols[0] if cols else None,
                            "y_col": cols[1] if len(cols) > 1 else (cols[0] if cols else None),
                            "area": 1.0, "use_density": False,
                            "mppt_x": 10, "mppt_y": -30,
                            "fill_area": False
                        })
                        st.success("è¿½åŠ ã—ã¾ã—ãŸ")
                        st.rerun()
                except Exception as e: st.error(f"ã‚¨ãƒ©ãƒ¼ (TabåŒºåˆ‡ã‚Šã¨ã—ã¦èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ): {e}")

    datasets = st.session_state['gp_data_list']
    if not datasets: return

    # ==========================================
    # 2. ã‚°ãƒ©ãƒ•è¨­å®š
    # ==========================================
    st.markdown("---")
    col_settings, col_preview = st.columns([1.3, 2])

    with col_settings:
        st.subheader("2. è©³ç´°è¨­å®š")
        
        # --- A. ã‚­ãƒ£ãƒ³ãƒã‚¹ (cmæŒ‡å®š) ---
        with st.expander("ğŸ“Š ã‚­ãƒ£ãƒ³ãƒã‚¹ãƒ»ãƒ•ã‚©ãƒ³ãƒˆ", expanded=False):
            c1, c2 = st.columns(2)
            fig_w_cm = c1.number_input("å¹… (cm)", 2.0, 100.0, 15.0, step=0.5, key="fw_cm_v20")
            fig_h_cm = c2.number_input("é«˜ã• (cm)", 2.0, 100.0, 10.0, step=0.5, key="fh_cm_v20")
            
            fig_w_inch = fig_w_cm / 2.54
            fig_h_inch = fig_h_cm / 2.54
            
            dpi_val = st.number_input("è§£åƒåº¦ (DPI)", 72, 600, 150, key="dpi_in_v20")
            font_family = st.selectbox("ãƒ•ã‚©ãƒ³ãƒˆ", ["Times New Roman", "Arial", "Helvetica", "Meiryo", "Yu Gothic"], index=0, key="ff_sel_v20")
            base_fs = st.number_input("åŸºæœ¬ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º", 6, 50, 12, key="bfs_in_v20")

        # --- B. è»¸è¨­å®š ---
        with st.expander("ğŸ“ è»¸ (Axes) ã¨ å˜ä½å¤‰æ›", expanded=True):
            tabs_ax = st.tabs(["Xè»¸(ä¸‹)", "Xè»¸(ä¸Š)", "Yè»¸(å·¦)", "Yè»¸(å³)", "å…±é€š"])
            ax_settings = {}
            SCALE_OPTIONS = {
                "x1 (ãã®ã¾ã¾)": 1.0, "x1000 (m)": 1000.0, "x10^6 (Âµ)": 1e6,
                "x10^9 (n)": 1e9, "x10^12 (p)": 1e12, "x0.001 (k)": 0.001
            }

            def axis_ui(key_prefix, label_def, use_top=False, use_right=False):
                col_btn = st.columns(3)
                if col_btn[0].button("Voltage(V)", key=f"p_v_{key_prefix}"):
                    st.session_state[f"{key_prefix}_lbl_v20"] = "Voltage (V)"
                    st.session_state[f"{key_prefix}_scale_idx_v20"] = 0
                if col_btn[1].button("Current(mA)", key=f"p_ma_{key_prefix}"):
                    st.session_state[f"{key_prefix}_lbl_v20"] = "Current (mA)"
                    st.session_state[f"{key_prefix}_scale_idx_v20"] = 1
                if col_btn[2].button("Current(ÂµA)", key=f"p_ua_{key_prefix}"):
                    st.session_state[f"{key_prefix}_lbl_v20"] = "Current (ÂµA)"
                    st.session_state[f"{key_prefix}_scale_idx_v20"] = 2

                label = st.text_input("ãƒ©ãƒ™ãƒ«", label_def, key=f"{key_prefix}_lbl_v20")
                
                curr_idx = st.session_state.get(f"{key_prefix}_scale_idx_v20", 0)
                scale_key = st.selectbox("è¡¨ç¤ºå€ç‡", list(SCALE_OPTIONS.keys()), index=curr_idx, key=f"{key_prefix}_scale_sel_v20")
                st.session_state[f"{key_prefix}_scale_idx_v20"] = list(SCALE_OPTIONS.keys()).index(scale_key)
                
                current_scale_val = SCALE_OPTIONS[scale_key]
                prev_scale_key = f"{key_prefix}_prev_scale_val"
                prev_scale_val = st.session_state.get(prev_scale_key, 1.0)
                
                if current_scale_val != prev_scale_val:
                    ratio = current_scale_val / prev_scale_val
                    k_min = f"{key_prefix}_min_v20"
                    k_max = f"{key_prefix}_max_v20"
                    if st.session_state.get(k_min) is not None:
                        st.session_state[k_min] = st.session_state[k_min] * ratio
                    if st.session_state.get(k_max) is not None:
                        st.session_state[k_max] = st.session_state[k_max] * ratio
                    st.session_state[prev_scale_key] = current_scale_val

                data_vals = []
                for d in datasets:
                    if not d.get('visible', True): continue
                    if d.get('x_col') is None or d.get('y_col') is None: continue
                    if d['x_col'] not in d['df'].columns or d['y_col'] not in d['df'].columns: continue

                    is_this_axis_x = (d.get('use_top', False) == use_top)
                    is_this_axis_y = (d.get('use_right', False) == use_right)
                    
                    val = None
                    if key_prefix.startswith('x') and is_this_axis_x:
                        val = d['df'][d['x_col']]
                    elif key_prefix.startswith('y') and is_this_axis_y:
                        val = d['df'][d['y_col']]
                        if d.get('use_density', False) and d.get('area', 1.0) > 0:
                            val = val / d['area']

                    if val is not None:
                        data_vals.append(val * current_scale_val)
                
                calc_min, calc_max = None, None
                if data_vals:
                    concat_data = pd.concat(data_vals)
                    if not concat_data.empty:
                        calc_min = float(concat_data.min())
                        calc_max = float(concat_data.max())
                        margin = (calc_max - calc_min) * 0.05
                        if margin == 0: margin = abs(calc_max) * 0.1 if calc_max!=0 else 1.0
                        calc_min -= margin
                        calc_max += margin

                k_min = f"{key_prefix}_min_v20"
                k_max = f"{key_prefix}_max_v20"
                if st.session_state.get(k_min) is None and calc_min is not None:
                    st.session_state[k_min] = calc_min
                if st.session_state.get(k_max) is None and calc_max is not None:
                    st.session_state[k_max] = calc_max

                if prev_scale_key not in st.session_state:
                     st.session_state[prev_scale_key] = current_scale_val

                c1, c2 = st.columns(2)
                d_min = c1.number_input("æœ€å°", value=None, format="%f", key=k_min)
                d_max = c2.number_input("æœ€å¤§", value=None, format="%f", key=k_max)
                c3, c4 = st.columns(2)
                maj_int = c3.number_input("ä¸»ç›®ç››", 0.0, step=0.1, key=f"{key_prefix}_maj_v20")
                min_int = c4.number_input("è£œåŠ©ç›®ç››", 0.0, step=0.1, key=f"{key_prefix}_min_int_v20")
                
                c5, c6 = st.columns(2)
                is_log = c5.checkbox("å¯¾æ•°è»¸", False, key=f"{key_prefix}_log_v20")
                is_inv = c6.checkbox("è»¸ã‚’åè»¢", False, key=f"{key_prefix}_inv_v20")

                return {"label": label, "min": d_min, "max": d_max, "maj": maj_int, "log": is_log, "inv": is_inv, "scale": current_scale_val}

            with tabs_ax[0]: ax_settings['x1'] = axis_ui("x1", "Voltage (V)", use_top=False)
            with tabs_ax[1]: ax_settings['x2'] = axis_ui("x2", "Secondary X", use_top=True)
            with tabs_ax[2]: ax_settings['y1'] = axis_ui("y1", "Current (A)", use_right=False)
            with tabs_ax[3]: ax_settings['y2'] = axis_ui("y2", "Power (W)", use_right=True)
            
            with tabs_ax[4]:
                tick_dir = st.selectbox("ç›®ç››ã®å‘ã", ["in", "out", "inout"], index=0, key="tdir_v20")
                show_grid = st.checkbox("ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤º", False, key="sgrid_v20")
                zero_cross = st.checkbox("åŸç‚¹ç·šæç”»", True, key="zcross_v20")

        # --- C. å‡¡ä¾‹è¨­å®š ---
        with st.expander("ğŸ“ å‡¡ä¾‹ (Legend)", expanded=True):
            show_leg = st.checkbox("å‡¡ä¾‹ã‚’è¡¨ç¤º", True, key="sleg_v20")
            
            st.markdown("#### å‡¡ä¾‹é †åºãƒ»è¡¨ç¤ºè¨­å®š")
            for i, d in enumerate(datasets):
                did = d['id']
                c_vis, c_name, c_up, c_down = st.columns([0.5, 4, 0.7, 0.7])
                with c_vis:
                    d['visible'] = st.checkbox("vis", value=d.get('visible', True), key=f"vis_main_{did}", label_visibility="collapsed")
                with c_name:
                    st.text(f"{d.get('legend_name', d['name'])}")
                with c_up:
                    if st.button("â¬†", key=f"leg_u_{did}"): move_data(i, "up"); st.rerun()
                with c_down:
                    if st.button("â¬‡", key=f"leg_d_{did}"): move_data(i, "down"); st.rerun()

            if show_leg:
                st.markdown("---")
                st.markdown("**ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š**")
                c_auto, c_size = st.columns(2)
                auto_leg_size = c_auto.checkbox("ã‚µã‚¤ã‚ºè‡ªå‹•èª¿æ•´", True, key="auto_leg_size_v20")
                manual_fs = c_size.number_input("ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º", 5, 40, int(base_fs), disabled=auto_leg_size, key="lfont_v20")
                if auto_leg_size:
                    l_fontsize = max(6, int(base_fs) - (len(datasets) // 3))
                else:
                    l_fontsize = manual_fs

                c1, c2 = st.columns(2)
                l_loc = c1.selectbox("ä½ç½®", ["best", "upper right", "upper left", "lower right", "lower left", "outside right"], index=0, key="lloc_v20")
                l_col = c2.number_input("åˆ—æ•°", 1, 5, 1, key="lcol_v20")
                l_frame = st.checkbox("æ ç·šã‚’è¡¨ç¤º", False, key="lframe_v20")

        # --- D. ãƒ‡ãƒ¼ã‚¿ç³»åˆ— ---
        st.markdown("#### ãƒ‡ãƒ¼ã‚¿ç³»åˆ—è¨­å®š")
        
        for i, d in enumerate(datasets):
            did = d['id']
            with st.expander(f"#{i+1}: {d.get('legend_name', d['name'])}", expanded=False):
                d['legend_name'] = st.text_input("å‡¡ä¾‹è¡¨ç¤ºå", value=d.get('legend_name', d['name']), key=f"leg_nm_{did}")

                # æ“ä½œãƒœã‚¿ãƒ³ï¼ˆè¤‡è£½ã‚’è¿½åŠ ï¼‰
                bc1, bc2, bc3, bc4 = st.columns([1, 1, 1.5, 2])
                with bc1:
                    if st.button("â¬†", key=f"btn_u_{did}"): move_data(i, "up"); st.rerun()
                with bc2:
                    if st.button("â¬‡", key=f"btn_d_{did}"): move_data(i, "down"); st.rerun()
                with bc3:
                    if st.button("Â©ï¸ è¤‡è£½", key=f"btn_dup_{did}", help="ã“ã®ãƒ‡ãƒ¼ã‚¿ç³»åˆ—ã‚’è¤‡è£½ã—ã¦è¿½åŠ ã—ã¾ã™"): duplicate_data(i); st.rerun()
                with bc4:
                    if st.button("âŒ å‰Šé™¤", key=f"btn_del_{did}"): datasets.pop(i); st.rerun()

                cols = d['df'].columns.tolist()
                sc1, sc2, sc3 = st.columns([2, 2, 1])
                curr_xc = d.get('x_col')
                curr_yc = d.get('y_col')
                ix_x = cols.index(curr_xc) if curr_xc in cols else 0
                ix_y = cols.index(curr_yc) if curr_yc in cols else (1 if len(cols)>1 else 0)

                xc = sc1.selectbox(f"Xåˆ—", cols, index=ix_x, key=f"xc_{did}")
                yc = sc2.selectbox(f"Yåˆ—", cols, index=ix_y, key=f"yc_{did}")
                # X/Y å…¥æ›¿
                if sc3.button("ğŸ”„ å…¥æ›¿", key=f"xy_swap_{did}"):
                    d['x_col'], d['y_col'] = yc, xc
                    st.rerun()

                st.caption("é›»æµå¯†åº¦è¨ˆç®— (Yè»¸ = Y / é¢ç©)")
                ac_dens1, ac_dens2 = st.columns(2)
                d['area'] = ac_dens1.number_input("ãƒ‡ãƒã‚¤ã‚¹é¢ç© (cmÂ²)", 0.0, 100.0, float(d.get('area', 1.0)), format="%.4f", key=f"area_{did}")
                d['use_density'] = ac_dens2.checkbox("é›»æµå¯†åº¦ã«æ›ç®—", d.get('use_density', False), key=f"dens_{did}")

                ac1, ac2 = st.columns(2)
                d['use_top'] = ac1.checkbox("ä¸ŠXè»¸", d.get('use_top', False), key=f"ut_{did}")
                d['use_right'] = ac2.checkbox("å³Yè»¸", d.get('use_right', False), key=f"ur_{did}")

                tc1, tc2 = st.columns(2)
                d['color'] = tc1.color_picker("è‰²", d.get('color', '#0000FF'), key=f"clr_{did}")
                d['marker'] = tc2.selectbox("ãƒãƒ¼ã‚«ãƒ¼", ["None", "o", "s", "^", "x"], index=0 if d.get('marker')=="None" else 1, key=f"mrk_{did}")
                
                lw1, lw2 = st.columns(2)
                d['line_width'] = lw1.number_input("ç·šå¹…", 0.0, 10.0, float(d.get('line_width', 1.5)), key=f"lw_{did}")
                d['marker_size'] = lw2.number_input("ç‚¹ã‚µã‚¤ã‚º", 0.0, 20.0, float(d.get('marker_size', 6.0)), key=f"ms_{did}")
                d['linestyle'] = st.selectbox("ç·šç¨®", ["-", "--", "-.", ":", "None"], index=0, key=f"lst_{did}")
                
                d['fill_area'] = st.checkbox("0ã¾ã§å¡—ã‚Šã¤ã¶ã™ (Fill)", d.get('fill_area', False), key=f"fill_{did}")

                st.markdown("---")
                d['mppt'] = st.checkbox("MPPTè§£æ", d.get('mppt', False), key=f"mppt_{did}")
                if d['mppt']:
                    mp1, mp2 = st.columns(2)
                    d['mppt_x'] = mp1.number_input("Text X Offset", value=int(d.get('mppt_x', 10)), key=f"mx_{did}")
                    d['mppt_y'] = mp2.number_input("Text Y Offset", value=int(d.get('mppt_y', -30)), key=f"my_{did}")

                d['fit_mode'] = st.selectbox("è¿‘ä¼¼æ›²ç·š", ["ãªã—", "ç·šå½¢", "å¤šé …å¼(2æ¬¡)", "ç§»å‹•å¹³å‡"], index=0, key=f"fit_{did}")
                if d['fit_mode'] != "ãªã—":
                    d['show_eq'] = st.checkbox("æ•°å¼ã‚’è¡¨ç¤º", d.get('show_eq', False), key=f"seq_{did}")

                d.update({'x_col': xc, 'y_col': yc})

    # ==========================================
    # 3. æç”»
    # ==========================================
    with col_preview:
        st.subheader("ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        
        # ãƒ•ã‚©ãƒ³ãƒˆ & æ•°å¼è¨­å®š
        if font_family in ["Times New Roman", "Times"]:
            plt.rcParams['font.family'] = 'serif'
            plt.rcParams['font.serif'] = [font_family] + plt.rcParams['font.serif']
        else:
            plt.rcParams['font.family'] = 'sans-serif'
            plt.rcParams['font.sans-serif'] = [font_family] + plt.rcParams['font.sans-serif']
        plt.rcParams['font.size'] = base_fs
        
        # MathTextãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹åŒ–
        plt.rcParams['axes.formatter.use_mathtext'] = True

        # CM -> Inch å¤‰æ›ã—ã¦Figureä½œæˆ
        fig, ax1 = plt.subplots(figsize=(fig_w_inch, fig_h_inch), dpi=dpi_val)
        
        visible_datasets = [d for d in datasets if d.get('visible', True)]
        
        has_right = any(d.get('use_right') for d in visible_datasets)
        has_top = any(d.get('use_top') for d in visible_datasets)

        ax2, ax3 = None, None
        axes_map = {(False, False): ax1}

        if has_right:
            ax2 = ax1.twinx()
            axes_map[(False, True)] = ax2
        if has_top:
            ax3 = ax1.twiny()
            axes_map[(True, False)] = ax3
        if has_right and has_top:
            axes_map[(True, True)] = ax3

        # è»¸è¨­å®šé©ç”¨é–¢æ•° (X, Y ç‹¬ç«‹ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼)
        def apply_axis_conf(ax, xc, yc):
            if not ax: return
            ax.set_xlabel(xc['label'])
            ax.set_ylabel(yc['label'])
            if xc['min'] is not None: ax.set_xlim(left=xc['min'])
            if xc['max'] is not None: ax.set_xlim(right=xc['max'])
            if yc['min'] is not None: ax.set_ylim(bottom=yc['min'])
            if yc['max'] is not None: ax.set_ylim(top=yc['max'])
            if xc['log']: ax.set_xscale('log')
            if yc['log']: ax.set_yscale('log')
            
            if xc.get('inv', False):
                ax.invert_xaxis()
            
            xfmt = ticker.ScalarFormatter(useMathText=True)
            xfmt.set_powerlimits((-2, 3))
            ax.xaxis.set_major_formatter(xfmt)
            
            yfmt = ticker.ScalarFormatter(useMathText=True)
            yfmt.set_powerlimits((-2, 3))
            ax.yaxis.set_major_formatter(yfmt)

            if xc['maj'] > 0: ax.xaxis.set_major_locator(ticker.MultipleLocator(xc['maj']))
            if yc['maj'] > 0: ax.yaxis.set_major_locator(ticker.MultipleLocator(yc['maj']))
            ax.tick_params(direction=tick_dir, which='both')

        apply_axis_conf(ax1, ax_settings['x1'], ax_settings['y1'])
        apply_axis_conf(ax2, ax_settings['x1'], ax_settings['y2'])
        apply_axis_conf(ax3, ax_settings['x2'], ax_settings['y1'])
        
        if show_grid: ax1.grid(True, linestyle=':', alpha=0.6)
        if zero_cross: 
            ax1.axhline(0, color='black', linewidth=0.8)
            ax1.axvline(0, color='black', linewidth=0.8)

        legend_handles = []
        legend_labels = []

        for d in datasets:
            if not d.get('visible', True): continue
            
            if not d.get('x_col') or not d.get('y_col'): continue
            
            df = d['df']
            x_raw = df[d['x_col']]
            y_val = df[d['y_col']]
            
            if d.get('use_density', False) and d.get('area', 1.0) > 0:
                y_val = y_val / d['area']

            use_t = d.get('use_top', False)
            use_r = d.get('use_right', False)
            x_scale = ax_settings['x2']['scale'] if use_t else ax_settings['x1']['scale']
            y_scale = ax_settings['y2']['scale'] if use_r else ax_settings['y1']['scale']

            x_data = x_raw * x_scale
            y_data = y_val * y_scale
            
            target_ax = axes_map.get((use_t, use_r), ax1)
            
            mask = pd.notna(x_data) & pd.notna(y_data)
            x_plot, y_plot = x_data[mask], y_data[mask]
            if len(x_plot) == 0: continue

            ls = d.get('linestyle', '-')
            if ls == "None": ls = ""
            mk = d.get('marker', 'None')
            if mk == "None": mk = ""
            
            label_text = d.get('legend_name', d['name'])

            lines = target_ax.plot(x_plot, y_plot, label=label_text, 
                                   color=d['color'], marker=mk, linestyle=ls,
                                   linewidth=d.get('line_width', 1.5), markersize=d.get('marker_size', 6))
            
            if d.get('fill_area', False):
                target_ax.fill_between(x_plot, y_plot, 0, color=d['color'], alpha=0.3)

            if lines:
                legend_handles.append(lines[0])
                legend_labels.append(label_text)

            fmode = d.get('fit_mode', "ãªã—")
            if fmode != "ãªã—" and len(x_plot) > 1:
                try:
                    idx_sorted = np.argsort(x_plot)
                    xs = x_plot.iloc[idx_sorted]
                    ys = y_plot.iloc[idx_sorted]
                    y_fit = None
                    eq_text = ""
                    if "ç·šå½¢" in fmode:
                        slope, intercept, r_val, _, _ = stats.linregress(xs, ys)
                        y_fit = slope * xs + intercept
                        eq_text = f"y={slope:.2e}x+{intercept:.2e}\n$R^2$={r_val**2:.3f}"
                    elif "2æ¬¡" in fmode:
                        coef = np.polyfit(xs, ys, 2)
                        y_fit = np.polyval(coef, xs)
                        eq_text = "Poly(deg=2)"
                    elif "ç§»å‹•å¹³å‡" in fmode:
                        y_fit = ys.rolling(window=5, center=True).mean()

                    if y_fit is not None:
                        target_ax.plot(xs, y_fit, color=d['color'], linestyle='--', linewidth=1, alpha=0.8)
                        if d.get('show_eq') and eq_text:
                            target_ax.text(xs.iloc[-1], y_fit.iloc[-1], eq_text, fontsize=9, color=d['color'])
                except: pass

            if d.get('mppt'):
                m_mask = (x_plot < 0) & (y_plot > 0)
                xm, ym = x_plot[m_mask], y_plot[m_mask]
                if len(xm) > 0:
                    p_calc = (xm * ym).abs()
                    max_i = p_calc.idxmax()
                    best_p = p_calc[max_i]
                    best_x_plot = xm[max_i]
                    best_y_plot = ym[max_i]
                    pow_str = format_power(best_p)

                    target_ax.plot(best_x_plot, best_y_plot, marker='*', color='gold', markersize=14, markeredgecolor='black', zorder=10)
                    off_x = d.get('mppt_x', 10)
                    off_y = d.get('mppt_y', -30)
                    target_ax.annotate(f"MPPT: {pow_str}", xy=(best_x_plot, best_y_plot), xytext=(off_x, off_y),
                                       textcoords='offset points', arrowprops=dict(arrowstyle="->"),
                                       bbox=dict(boxstyle="round", fc="white", alpha=0.7))

        if show_leg and legend_handles:
            bbox = None
            loc_param = l_loc
            if l_loc == "outside right":
                loc_param = "center left"
                bbox = (1.05, 0.5)
            
            ax1.legend(legend_handles, legend_labels, 
                       loc=loc_param, bbox_to_anchor=bbox, ncol=l_col,
                       fontsize=l_fontsize, frameon=l_frame, edgecolor='black')

        plt.tight_layout()
        st.pyplot(fig, use_container_width=False)
        
        buf = BytesIO()
        fig.savefig(buf, format="png", dpi=300, bbox_inches='tight')
        
        default_img_name = f"plot_{datetime.now().strftime('%Y%m%d_%H%M')}"
        save_name_img = st.text_input("ç”»åƒä¿å­˜å (æ‹¡å¼µå­ä¸è¦)", value=default_img_name, key="img_save_name_v20")
        
        final_img_fname = save_name_img.strip()
        if not final_img_fname: final_img_fname = default_img_name
        if not final_img_fname.endswith(".png"): final_img_fname += ".png"

        st.download_button("ç”»åƒã‚’ä¿å­˜ (PNG)", buf.getvalue(), final_img_fname, "image/png", key="dl_png_v20")
# ---------------------------
# --- Components ---
# ---------------------------
# (å‰å›ã¨åŒã˜ page_data_list ã¯çœç•¥ã›ãšãã®ã¾ã¾è¨˜è¿°ã—ã¾ã™)
def page_data_list(sheet_name, title, col_time, col_filter, col_memo, col_url, detail_cols, col_filename):
    st.subheader(f"ğŸ“š {title} ä¸€è¦§")
    df = get_sheet_as_df(SPREADSHEET_NAME, sheet_name)
    if df.empty:
        st.info("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    search_query = st.text_input("ğŸ“ æ¤œç´¢ï¼ˆãƒ¡ãƒ¢/ã‚¿ã‚¤ãƒˆãƒ«ã‚’çµã‚Šè¾¼ã¿ï¼‰", key=f"{sheet_name}_search").strip()
    
    filtered_df = df.copy()
    if col_filter and col_filter in df.columns:
        options = ["ã™ã¹ã¦"] + sorted(list(df[col_filter].unique()))
        sel = st.selectbox(f"ã‚«ãƒ†ã‚´ãƒªã§çµã‚Šè¾¼ã¿", options)
        if sel != "ã™ã¹ã¦": filtered_df = filtered_df[filtered_df[col_filter] == sel]
            
    if search_query:
        searchable_cols = [col_memo]
        search_mask = False
        for col in searchable_cols:
            if col in filtered_df.columns:
                mask = filtered_df[col].astype(str).str.contains(search_query, case=False, na=False)
                search_mask = search_mask | mask
        filtered_df = filtered_df[search_mask]
        
    if filtered_df.empty:
        st.warning("è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    if col_time in filtered_df.columns:
        filtered_df = filtered_df.sort_values(col_time, ascending=False)

    st.markdown("---")
    for i, row in filtered_df.iterrows():
        ts_display = row.get(col_time,'ä¸æ˜')
        memo_content = str(row.get(col_memo,''))
        first_line = memo_content.split('\n')[0].strip()
        expander_title = f"{first_line}"
        
        with st.expander(expander_title):
            st.write(f"**{EPI_COL_TIMESTAMP}:** {ts_display}")
            for col in detail_cols:
                if col in row and col not in [col_url, col_filename, col_time]:
                    st.write(f"**{col}:** {row[col]}")
            display_attached_files(row, col_url, col_filename)

# ---------------------------
# --- Pages (Existing) ---
# ---------------------------
def page_epi_note_recording():
    st.markdown("#### ğŸ“ æ–°ã—ã„ã‚¨ãƒ”ãƒãƒ¼ãƒˆã‚’è¨˜éŒ²")
    with st.form("epi_form"):
        title = st.text_input("ã‚¿ã‚¤ãƒˆãƒ«/ç•ªå· (ä¾‹: 791)")
        cat = st.selectbox("ã‚«ãƒ†ã‚´ãƒª", ["D1", "D2", "ãã®ä»–"])
        memo = st.text_area("ãƒ¡ãƒ¢")
        files = st.file_uploader("æ·»ä»˜", accept_multiple_files=True)
        if st.form_submit_button("ä¿å­˜"):
            if not title: st.error("ã‚¿ã‚¤ãƒˆãƒ«å¿…é ˆ"); return
            f_names, f_urls = [], []
            if files:
                for f in files:
                    n, u = upload_file_to_gcs(storage_client, f)
                    if u: f_names.append(n); f_urls.append(u)
            row = [
                datetime.now().strftime("%Y%m%d_%H%M%S"),
                "ã‚¨ãƒ”ãƒãƒ¼ãƒˆ", cat, f"{title}\n{memo}",
                json.dumps(f_names), json.dumps(f_urls)
            ]
            try:
                gc.open(SPREADSHEET_NAME).worksheet(SHEET_EPI_DATA).append_row(row)
                st.success("ä¿å­˜æˆåŠŸ")
                st.cache_data.clear()
            except Exception as e: st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

def page_epi_note():
    st.header("ã‚¨ãƒ”ãƒãƒ¼ãƒˆ")
    tab1, tab2 = st.tabs(["ğŸ“ è¨˜éŒ²", "ğŸ“š ä¸€è¦§"])
    with tab1: page_epi_note_recording()
    with tab2:
        page_data_list(SHEET_EPI_DATA, "ã‚¨ãƒ”ãƒãƒ¼ãƒˆ", EPI_COL_TIMESTAMP, EPI_COL_CATEGORY, EPI_COL_MEMO, EPI_COL_FILE_URL, 
                       [EPI_COL_TIMESTAMP, EPI_COL_CATEGORY, EPI_COL_MEMO], EPI_COL_FILENAME)

def page_mainte_recording():
    st.markdown("#### ğŸ“ æ–°ã—ã„ãƒ¡ãƒ³ãƒ†ãƒãƒ¼ãƒˆã‚’è¨˜éŒ²")
    with st.form("mainte_form"):
        dev = st.selectbox("è£…ç½®", ["MBE", "XRD", "PL", "AFM", "ãã®ä»–"])
        title = st.text_input("ã‚¿ã‚¤ãƒˆãƒ«")
        memo = st.text_area("è©³ç´°")
        files = st.file_uploader("æ·»ä»˜", accept_multiple_files=True)
        if st.form_submit_button("ä¿å­˜"):
            if not title: st.error("ã‚¿ã‚¤ãƒˆãƒ«å¿…é ˆ"); return
            f_names, f_urls = [], []
            if files:
                for f in files:
                    n, u = upload_file_to_gcs(storage_client, f)
                    if u: f_names.append(n); f_urls.append(u)
            row = [
                datetime.now().strftime("%Y%m%d_%H%M%S"),
                "ãƒ¡ãƒ³ãƒ†ãƒãƒ¼ãƒˆ", f"[{title}] {dev}\n{memo}",
                json.dumps(f_names), json.dumps(f_urls)
            ]
            try:
                gc.open(SPREADSHEET_NAME).worksheet(SHEET_MAINTE_DATA).append_row(row)
                st.success("ä¿å­˜æˆåŠŸ")
                st.cache_data.clear()
            except Exception as e: st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

def page_mainte_note():
    st.header("ãƒ¡ãƒ³ãƒ†ãƒãƒ¼ãƒˆ")
    tab1, tab2 = st.tabs(["ğŸ“ è¨˜éŒ²", "ğŸ“š ä¸€è¦§"])
    with tab1: page_mainte_recording()
    with tab2:
        page_data_list(SHEET_MAINTE_DATA, "ãƒ¡ãƒ³ãƒ†ãƒãƒ¼ãƒˆ", MAINT_COL_TIMESTAMP, None, MAINT_COL_MEMO, MAINT_COL_FILE_URL,
                       [MAINT_COL_TIMESTAMP, MAINT_COL_MEMO], MAINT_COL_FILENAME)

def page_meeting_note():
    st.header("è­°äº‹éŒ²")
    with st.form("meeting_form"):
        title = st.text_input("ä¼šè­°ã‚¿ã‚¤ãƒˆãƒ«")
        content = st.text_area("å†…å®¹")
        url = st.text_input("éŸ³å£°URL")
        if st.form_submit_button("ä¿å­˜"):
            if not title: st.error("ã‚¿ã‚¤ãƒˆãƒ«å¿…é ˆ"); return
            row = [datetime.now().strftime("%Y%m%d_%H%M%S"), title, "", url, content]
            try:
                gc.open(SPREADSHEET_NAME).worksheet(SHEET_MEETING_DATA).append_row(row)
                st.success("ä¿å­˜æˆåŠŸ")
                st.cache_data.clear()
            except Exception as e: st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
    page_data_list(SHEET_MEETING_DATA, "è­°äº‹éŒ²", MEETING_COL_TIMESTAMP, None, MEETING_COL_TITLE, MEETING_COL_AUDIO_URL,
                   [MEETING_COL_TIMESTAMP, MEETING_COL_TITLE, MEETING_COL_CONTENT], None)

def page_qa_box():
    st.header("çŸ¥æµè¢‹")
    with st.form("qa_form"):
        title = st.text_input("è³ªå•ã‚¿ã‚¤ãƒˆãƒ«")
        content = st.text_area("å†…å®¹")
        contact = st.text_input("é€£çµ¡å…ˆ")
        files = st.file_uploader("æ·»ä»˜", accept_multiple_files=True)
        if st.form_submit_button("é€ä¿¡"):
            if not title: st.error("ã‚¿ã‚¤ãƒˆãƒ«å¿…é ˆ"); return
            f_names, f_urls = [], []
            if files:
                for f in files:
                    n, u = upload_file_to_gcs(storage_client, f)
                    if u: f_names.append(n); f_urls.append(u)
            row = [
                datetime.now().strftime("%Y%m%d_%H%M%S"), title, content, contact,
                json.dumps(f_names), json.dumps(f_urls), "æœªè§£æ±º"
            ]
            try:
                gc.open(SPREADSHEET_NAME).worksheet(SHEET_QA_DATA).append_row(row)
                st.success("é€ä¿¡æˆåŠŸ")
                st.cache_data.clear()
            except Exception as e: st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
    page_data_list(SHEET_QA_DATA, "QA", QA_COL_TIMESTAMP, QA_COL_STATUS, QA_COL_TITLE, QA_COL_FILE_URL,
                   [QA_COL_TIMESTAMP, QA_COL_TITLE, QA_COL_CONTENT, QA_COL_STATUS], QA_COL_FILENAME)

def page_handover_note():
    st.header("å¼•ãç¶™ããƒ¡ãƒ¢")
    with st.form("handover_form"):
        htype = st.selectbox("ç¨®é¡", ["ãƒãƒ‹ãƒ¥ã‚¢ãƒ«", "è£…ç½®è¨­å®š", "ãã®ä»–"])
        title = st.text_input("ã‚¿ã‚¤ãƒˆãƒ«")
        memo = st.text_area("å†…å®¹")
        if st.form_submit_button("ä¿å­˜"):
            if not title: st.error("ã‚¿ã‚¤ãƒˆãƒ«å¿…é ˆ"); return
            try:
                gc.open(SPREADSHEET_NAME).worksheet(SHEET_HANDOVER_DATA).append_row([
                    datetime.now().strftime("%Y%m%d_%H%M%S"), htype, title, memo
                ])
                st.success("ä¿å­˜æˆåŠŸ")
                st.cache_data.clear()
            except Exception as e: st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
    page_data_list(SHEET_HANDOVER_DATA, "å¼•ãç¶™ã", HANDOVER_COL_TIMESTAMP, HANDOVER_COL_TYPE, HANDOVER_COL_TITLE, None,
                   [HANDOVER_COL_TIMESTAMP, HANDOVER_COL_TYPE, HANDOVER_COL_TITLE, HANDOVER_COL_MEMO], None)

def page_trouble_report():
    st.header("ãƒˆãƒ©ãƒ–ãƒ«å ±å‘Š")
    with st.form("trouble_form"):
        dev = st.selectbox("æ©Ÿå™¨", ["MBE", "XRD", "PL", "IV", "TEMãƒ»SEM", "æŠµæŠ—åŠ ç†±è’¸ç€", "RTA", "ãƒ•ã‚©ãƒˆãƒªã‚½", "ãƒ‰ãƒ©ãƒ•ãƒˆ", "ãã®ä»–"])
        title = st.text_input("ä»¶å")
        cause = st.text_area("åŸå› ")
        sol = st.text_area("å¯¾ç­–")
        rep = st.text_input("å ±å‘Šè€…")
        if st.form_submit_button("ä¿å­˜"):
            try:
                gc.open(SPREADSHEET_NAME).worksheet(SHEET_TROUBLE_DATA).append_row([
                    datetime.now().strftime("%Y%m%d_%H%M%S"), dev, "", "", cause, sol, "", rep, "", "", title
                ])
                st.success("ä¿å­˜æˆåŠŸ")
                st.cache_data.clear()
            except Exception as e: st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
    page_data_list(SHEET_TROUBLE_DATA, "ãƒˆãƒ©ãƒ–ãƒ«", TROUBLE_COL_TIMESTAMP, TROUBLE_COL_DEVICE, TROUBLE_COL_TITLE, None,
                   [TROUBLE_COL_TIMESTAMP, TROUBLE_COL_DEVICE, TROUBLE_COL_TITLE, TROUBLE_COL_CAUSE, TROUBLE_COL_SOLUTION], None)

def page_contact_form():
    st.header("ãŠå•ã„åˆã‚ã›")
    with st.form("contact_form"):
        ctype = st.selectbox("ç¨®é¡", ["ãƒã‚°å ±å‘Š", "æ©Ÿèƒ½è¦æœ›", "ãƒ‡ãƒ¼ã‚¿ä¿®æ­£ä¾é ¼", "ãã®ä»–"])
        detail = st.text_area("è©³ç´°")
        contact = st.text_input("é€£çµ¡å…ˆ")
        if st.form_submit_button("é€ä¿¡"):
            if not detail: st.error("è©³ç´°å¿…é ˆ"); return
            try:
                gc.open(SPREADSHEET_NAME).worksheet(SHEET_CONTACT_DATA).append_row([
                    datetime.now().strftime("%Y%m%d_%H%M%S"), ctype, detail, contact
                ])
                st.success("é€ä¿¡æˆåŠŸ")
                st.cache_data.clear()
            except Exception as e: st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

# ---------------------------
# --- Analysis Pages (Original IV/PL) ---
# ---------------------------
# (IVã¨PLã¯å‰å›ã®æœ€çµ‚ä¿®æ­£ç‰ˆã‚’ãã®ã¾ã¾æ­è¼‰ã—ã¾ã™)

def page_iv_analysis():
    st.header("âš¡ IVãƒ‡ãƒ¼ã‚¿è§£æ")
    use_log_scale = st.checkbox("ç¸¦è»¸ï¼ˆé›»æµï¼‰ã‚’å¯¾æ•°è¡¨ç¤ºã«ã™ã‚‹", key="iv_log_scale")
    files = st.file_uploader("IVãƒ•ã‚¡ã‚¤ãƒ«(.txt)", accept_multiple_files=True)
    
    data_for_export = []
    dfs_to_plot = []
    
    if files:
        with st.spinner("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ã‚°ãƒ©ãƒ•ã‚’æº–å‚™ä¸­..."):
            fig, ax = plt.subplots(figsize=(8, 6))
            has_plot = False
            
            for f in files:
                df = load_data_file(f.getvalue(), f.name)
                if df is not None:
                    data_for_export.append(df)
                    plot_df = df.copy()
                    if use_log_scale:
                        plot_df.iloc[:, 1] = np.abs(plot_df.iloc[:, 1])
                    dfs_to_plot.append(plot_df)
                    has_plot = True

            for plot_df in dfs_to_plot:
                ax.plot(plot_df['Axis_X'], plot_df.iloc[:,1], label=plot_df.columns[1])

        if has_plot:
            if use_log_scale:
                ax.set_yscale('log')
                st.warning("âš ï¸ å¯¾æ•°è¡¨ç¤ºã®ãŸã‚ã€é›»æµå€¤ã¯**çµ¶å¯¾å€¤**ã«å¤‰æ›ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ã„ã¾ã™ã€‚")
            else:
                ax.set_yscale('linear')
            if not use_log_scale:
                 ax.axhline(0, color='gray', linestyle='--', linewidth=1)
            ax.axvline(0, color='gray', linestyle='--', linewidth=1)
            ax.set_xlabel("Voltage")
            ax.set_ylabel("Current")
            ax.legend()
            ax.grid(True, linestyle=':', alpha=0.5)
            st.pyplot(fig)
            
            st.markdown("---")
            st.subheader("ğŸ“¥ è§£æçµæœã®ã‚¨ã‚¯ã‚»ãƒ«å‡ºåŠ›")
            
            if data_for_export:
                is_consistent = False
                if len(data_for_export) > 0:
                    ref_df = data_for_export[0]
                    ref_x_vals = ref_df['Axis_X'].to_numpy()
                    ref_min, ref_max, ref_len = ref_x_vals.min(), ref_x_vals.max(), len(ref_x_vals)
                    all_match = True
                    for df in data_for_export[1:]:
                        df_x_vals = df['Axis_X'].to_numpy()
                        if not (np.isclose(df_x_vals.min(), ref_min) and np.isclose(df_x_vals.max(), ref_max) and len(df_x_vals) == ref_len):
                            all_match = False; break
                    is_consistent = all_match

                if is_consistent and len(data_for_export) > 1:
                    st.success("âœ… å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®é›»åœ§è»¸ãŒä¸€è‡´ã™ã‚‹ãŸã‚ã€**æ¸¬å®šé †åºã‚’ä¿æŒ**ã—ãŸã¾ã¾1æšã®ã‚·ãƒ¼ãƒˆã«çµ±åˆã—ã¾ã™ã€‚")
                    with st.spinner("Excelå‡ºåŠ›ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆä¸­ (é †åºä¿æŒ)..."):
                        dfs_to_concat = [data_for_export[0]]
                        for df in data_for_export[1:]:
                            dfs_to_concat.append(df[[df.columns[1]]])
                        merged_df = pd.concat(dfs_to_concat, axis=1)
                        excel_data = to_excel(merged_df)
                else:
                    data_for_export_dict = {}
                    with st.spinner("Excelå‡ºåŠ›ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ä¸­ (ã‚·ãƒ¼ãƒˆåˆ†å‰²)..."):
                        for df in data_for_export:
                            data_for_export_dict[df.columns[1].replace('.txt', '')] = df
                    if len(data_for_export) > 1:
                        st.warning("âš ï¸ é›»åœ§è»¸ã®ç¯„å›²ã‚„ã‚¹ãƒ†ãƒƒãƒ—ãŒç•°ãªã‚‹ãŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ã‚·ãƒ¼ãƒˆã‚’åˆ†ã‘ã¦å‡ºåŠ›ã—ã¾ã™ã€‚")
                        excel_data = to_excel_multi_sheet(data_for_export_dict)
                    else:
                         st.info("ãƒ•ã‚¡ã‚¤ãƒ«ãŒ1ã¤ã ã‘ã®ãŸã‚ã€1æšã®ã‚·ãƒ¼ãƒˆã«å‡ºåŠ›ã—ã¾ã™ã€‚")
                         excel_data = to_excel(data_for_export[0])
                
                default_name = datetime.now().strftime("IV_Analysis_%Y%m%d")
                filename_input = st.text_input("ãƒ•ã‚¡ã‚¤ãƒ«å (.xlsx)", value=default_name, key="iv_filename")
                st.download_button("Excelãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", excel_data, f"{filename_input}.xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", key="iv_download_btn")
        else:
            st.warning("ãƒ—ãƒ­ãƒƒãƒˆã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

def page_pl_analysis():
    st.header("ğŸ’¡ PLãƒ‡ãƒ¼ã‚¿è§£æ")
    if 'pl_slope' not in st.session_state: st.session_state['pl_slope'] = None
    if 'pl_center_wl' not in st.session_state: st.session_state['pl_center_wl'] = 1700

    st.markdown("## 1ï¸âƒ£ Step 1: æ³¢é•·æ ¡æ­£")
    st.info("2ã¤ã®æ—¢çŸ¥ã®æ³¢é•·ãƒ”ãƒ¼ã‚¯ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€æ ¡æ­£ä¿‚æ•°ã‚’æ±ºå®šã—ã¾ã™ã€‚")
    c1, c2 = st.columns(2)
    wl1 = c1.number_input("æ—¢çŸ¥æ³¢é•·1 (nm)", value=1500.0, key="wl1_input")
    wl2 = c2.number_input("æ—¢çŸ¥æ³¢é•·2 (nm)", value=1570.0, key="wl2_input")
    f1 = c1.file_uploader("æ³¢é•·1ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«", key="c1")
    f2 = c2.file_uploader("æ³¢é•·2ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«", key="c2")
    if f1 and f2:
        df1 = load_pl_data(f1)
        df2 = load_pl_data(f2)
        if df1 is not None and not df1.empty and df2 is not None and not df2.empty:
            try:
                p1 = df1.loc[df1['intensity'].idxmax(), 'pixel']
                p2 = df2.loc[df2['intensity'].idxmax(), 'pixel']
                if p1 != p2:
                    slope_raw = (wl2 - wl1) / (p2 - p1)
                    slope = np.abs(slope_raw)
                    st.success(f"âœ… è¨ˆç®—ã•ã‚ŒãŸæ ¡æ­£ä¿‚æ•° (nm/pixel): **{slope:.4f}**")
                    st.caption(f"ï¼ˆè¨ˆç®—å€¤: {slope_raw:.4f} nm/pixel ã®çµ¶å¯¾å€¤ã‚’å–å¾—ã—ã¾ã—ãŸã€‚ï¼‰")
                    if st.button("ã“ã®ä¿‚æ•°ã‚’ä¿å­˜ã—ã¦Step 2ã¸é€²ã‚€", key="save_slope"):
                        st.session_state['pl_slope'] = slope
                        st.rerun() 
                else: st.error("ãƒ”ãƒ¼ã‚¯ä½ç½®ãŒåŒã˜ã§ã™ã€‚")
            except Exception as e: st.error(f"è§£æã‚¨ãƒ©ãƒ¼: {e}")
        else: st.error("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    st.markdown("---")
    st.markdown("## 2ï¸âƒ£ Step 2: ä¸­å¿ƒæ³¢é•·ã®è¨­å®š")
    if st.session_state['pl_slope'] is None:
        st.warning("âš ï¸ ã¾ãš Step 1 ã§æ ¡æ­£ä¿‚æ•°ã‚’æ±ºå®šãƒ»ä¿å­˜ã—ã¦ãã ã•ã„ã€‚")
    else:
        st.success(f"æ ¡æ­£ä¿‚æ•°: {st.session_state['pl_slope']:.4f} nm/pixel ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚")
        center_wl = st.number_input("åˆ†å…‰å™¨ã®ä¸­å¿ƒæ³¢é•· (nm) ã‚’å…¥åŠ›", value=st.session_state['pl_center_wl'], key='center_wl_input')
        if st.button("ä¸­å¿ƒæ³¢é•·ã‚’ä¿å­˜ã—ã¦Step 3ã¸é€²ã‚€", key="save_center_wl"):
            st.session_state['pl_center_wl'] = center_wl
            st.rerun()

    st.markdown("---")
    st.markdown("## 3ï¸âƒ£ Step 3: æ¸¬å®šãƒ‡ãƒ¼ã‚¿è§£æå®Ÿè¡Œ")
    if st.session_state['pl_slope'] is None or st.session_state['pl_center_wl'] is None:
        st.warning("âš ï¸ Step 1 (æ ¡æ­£ä¿‚æ•°) ã¨ Step 2 (ä¸­å¿ƒæ³¢é•·) ã®ä¸¡æ–¹ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    else:
        slope = st.session_state['pl_slope']
        cw = st.session_state['pl_center_wl']
        st.info(f"ç¾åœ¨ã®è¨­å®š: ä¿‚æ•°={slope:.4f}, ä¸­å¿ƒæ³¢é•·={cw} nm")
        files = st.file_uploader("æ¸¬å®šãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«(.txt)", accept_multiple_files=True, key="pl_m")
        if files:
            fig, ax = plt.subplots(figsize=(10, 6))
            has_plot = False
            data_for_export = []
            for f in files:
                df = load_pl_data(f)
                if df is not None and not df.empty:
                    df['wl'] = (df['pixel'] - 256.5) * slope + cw
                    ax.plot(df['wl'], df['intensity'], label=f.name)
                    has_plot = True
                    export_df = df[['wl', 'intensity']].copy()
                    export_df.columns = [f"Wavelength ({f.name})", f"Intensity ({f.name})"]
                    data_for_export.append(export_df)
            
            if has_plot:
                ax.set_xlabel("Wavelength (nm)")
                ax.set_ylabel("Intensity (a.u.)")
                ax.legend()
                ax.grid(True, linestyle='--', alpha=0.7)
                st.pyplot(fig)
                
                st.markdown("---")
                st.subheader("ğŸ“¥ è§£æçµæœã®ã‚¨ã‚¯ã‚»ãƒ«å‡ºåŠ›")
                if data_for_export:
                    ref_wl_df = data_for_export[0].iloc[:, [0]].copy() 
                    ref_wl_df.columns = ['Wavelength_nm']
                    intensity_dfs = [df.iloc[:, [1]] for df in data_for_export] 
                    dfs_to_concat = [ref_wl_df] + intensity_dfs
                    merged_df = pd.concat(dfs_to_concat, axis=1)
                    default_name = datetime.now().strftime("PL_Analysis_%Y%m%d")
                    filename_input = st.text_input("ãƒ•ã‚¡ã‚¤ãƒ«å (.xlsx)", value=default_name, key="pl_filename")
                    excel_data = to_excel(merged_df)
                    st.download_button("Excelãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", excel_data, f"{filename_input}.xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", key="pl_download_btn")
            else:
                st.warning("ãƒ—ãƒ­ãƒƒãƒˆã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# ---------------------------
# --- Calendar ---
# ---------------------------
def page_calendar():
    st.header("ğŸ—“ï¸ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ»è£…ç½®äºˆç´„")
    
    st.subheader("å¤–éƒ¨äºˆç´„ã‚µã‚¤ãƒˆ")
    c1, c2 = st.columns(2)
    c1.markdown(f'<a href="https://www.eiiris.tut.ac.jp/evers/Web/dashboard.php" target="_blank"><button style="width:100%;padding:10px;background-color:#007BFF;color:white;border:none;border-radius:5px;">ğŸ”¬ Evers äºˆç´„ã‚µã‚¤ãƒˆã¸é£›ã¶</button></a>', unsafe_allow_html=True)
    c2.markdown(f'<a href="https://tech.rac.tut.ac.jp/regist/potal_0.php" target="_blank"><button style="width:100%;padding:10px;background-color:#28A745;color:white;border:none;border-radius:5px;">âš™ï¸ æ•™è‚²ç ”ç©¶åŸºç›¤ã‚»ãƒ³ã‚¿ãƒ¼ã¸é£›ã¶</button></a>', unsafe_allow_html=True)
    st.markdown("---")

    st.subheader("ç ”ç©¶å®¤ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼")
    src = CALENDAR_ID.replace("@", "%40")
    st.markdown(f'<iframe src="https://calendar.google.com/calendar/embed?src={src}&ctz=Asia%2FTokyo" style="border:0" width="100%" height="600" frameborder="0" scrolling="no"></iframe>', unsafe_allow_html=True)

    with st.expander("â• äºˆå®šã‚’è¿½åŠ "):
        with st.form("cal_form"):
            summ = st.text_input("ã‚¿ã‚¤ãƒˆãƒ«")
            sd = st.date_input("é–‹å§‹æ—¥"); st_time = st.time_input("é–‹å§‹æ™‚åˆ»")
            ed = st.time_input("çµ‚äº†æ™‚åˆ»")
            desc = st.text_area("è©³ç´°")
            if st.form_submit_button("äºˆç´„"):
                if calendar_service:
                    sdt = datetime.combine(sd, st_time).isoformat()
                    edt = datetime.combine(sd, ed).isoformat()
                    evt = {'summary': summ, 'description': desc, 
                           'start': {'dateTime': sdt, 'timeZone': 'Asia/Tokyo'},
                           'end': {'dateTime': edt, 'timeZone': 'Asia/Tokyo'}}
                    try:
                        calendar_service.events().insert(calendarId=CALENDAR_ID, body=evt).execute()
                        st.success("è¿½åŠ ã—ã¾ã—ãŸ")
                        st.rerun()
                    except Exception as e: st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
                else: st.error("ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼æ©Ÿèƒ½ç„¡åŠ¹")

# ---------------------------
# --- Main ---
# ---------------------------
def main():
    st.sidebar.title("Yamane Lab Tools")
    menu = st.sidebar.radio("ãƒ¡ãƒ‹ãƒ¥ãƒ¼", [
        "ã‚¨ãƒ”ãƒãƒ¼ãƒˆ", "ãƒ¡ãƒ³ãƒ†ãƒãƒ¼ãƒˆ", "ğŸ—“ï¸ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ»è£…ç½®äºˆç´„", 
        "IVãƒ‡ãƒ¼ã‚¿è§£æ", "PLãƒ‡ãƒ¼ã‚¿è§£æ", "ğŸ“ˆ é«˜æ©Ÿèƒ½ã‚°ãƒ©ãƒ•æç”»", 
        "è­°äº‹éŒ²", "çŸ¥æµè¢‹ãƒ»è³ªå•ç®±", "å¼•ãç¶™ããƒ¡ãƒ¢", "ãƒˆãƒ©ãƒ–ãƒ«å ±å‘Š", "ãŠå•ã„åˆã‚ã›"
    ])
    
    if 'curr_menu' not in st.session_state: st.session_state['curr_menu'] = menu
    if st.session_state['curr_menu'] != menu:
        st.cache_data.clear()
        st.session_state['curr_menu'] = menu

    if menu == "ã‚¨ãƒ”ãƒãƒ¼ãƒˆ": page_epi_note()
    elif menu == "ãƒ¡ãƒ³ãƒ†ãƒãƒ¼ãƒˆ": page_mainte_note()
    elif menu == "ğŸ—“ï¸ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ»è£…ç½®äºˆç´„": page_calendar()
    elif menu == "IVãƒ‡ãƒ¼ã‚¿è§£æ": page_iv_analysis()
    elif menu == "PLãƒ‡ãƒ¼ã‚¿è§£æ": page_pl_analysis()
    elif menu == "ğŸ“ˆ é«˜æ©Ÿèƒ½ã‚°ãƒ©ãƒ•æç”»": page_graph_plotting()
    elif menu == "è­°äº‹éŒ²": page_meeting_note()
    elif menu == "çŸ¥æµè¢‹ãƒ»è³ªå•ç®±": page_qa_box()
    elif menu == "å¼•ãç¶™ããƒ¡ãƒ¢": page_handover_note()
    elif menu == "ãƒˆãƒ©ãƒ–ãƒ«å ±å‘Š": page_trouble_report()
    elif menu == "ãŠå•ã„åˆã‚ã›": page_contact_form()

if __name__ == "__main__":
    try:
        if 'st.cache_data' in st.__dict__:
            st.cache_data.clear()
    except Exception:
        pass
    main()































